{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad108442",
   "metadata": {},
   "source": [
    "# Building the Entire RAG Ecosystem and Optimizing Every Component\n",
    "\n",
    "![alt text](architecture.png)\n",
    "\n",
    "1. **Query Transformations:** Rewriting user questions to be more effective for retrieval.\n",
    "2. **Intelligent Routing:** Directing a query to the correct data source or a specialized tool.\n",
    "3. **Indexing:** Creating a multi-layered knowledge base.\n",
    "4. **Retrieval and Re-ranking:** Filtering noise and prioritizing the most relevant context.\n",
    "5. **Self-Correcting Agentic Flows:** Building systems that can grade and improve their own work.\n",
    "6. **End-to-End Evaluation:** Objectively measuring the performance of the entire pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2639754",
   "metadata": {},
   "source": [
    "![ale text](simplerag.png)\n",
    "\n",
    "* **Indexing:** Organize and store data in a structured format to enable efficient searching.\n",
    "* **Retrieval:** Search and fetch relevant data based on a query or input.\n",
    "* **Generation:** Create a final response or output using the retrieved data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cf7009",
   "metadata": {},
   "source": [
    "### Indexing Phase\n",
    "\n",
    "![alt text](indexing.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f8d75740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc0d12dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Initialize a web document loader with specific parsing instructions\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-header\", \"post-title\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69197eea",
   "metadata": {},
   "source": [
    "We need to break the document into smaller, semantically meaningful pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c349e19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af91e503",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=OpenAIEmbeddings()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5c1b7d",
   "metadata": {},
   "source": [
    "### **Retrieval**\n",
    "\n",
    "The vector store is our library, and the retriever is our smart librarian. It takes a user’s query, embeds it, and then fetches the most semantically similar chunks from the vector store.\n",
    "\n",
    "![alt text](retrieval.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58137209",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07ff649a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asohi\\AppData\\Local\\Temp\\ipykernel_24624\\3864686756.py:1: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(\"What is Task Decomposition?\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n"
     ]
    }
   ],
   "source": [
    "docs = retriever.get_relevant_documents(\"What is Task Decomposition?\")\n",
    "\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c9824c",
   "metadata": {},
   "source": [
    "### Generator\n",
    "\n",
    "LLM to read it and formulate a human-friendly answer\n",
    "\n",
    "![alt text](generator.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f718e9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] input_types={} partial_variables={} metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52bfc23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c2006f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()} | prompt | llm | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5afd9473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task Decomposition is the process of breaking down a complicated task into smaller, manageable steps. This method often involves techniques like Chain of Thought (CoT) and Tree of Thoughts, which enhance reasoning and planning capabilities. It can be implemented through simple prompts, task-specific instructions, or external planning tools.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke(\"What is Task Decomposition?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7ce030",
   "metadata": {},
   "source": [
    "## Advanced Query Transformations\n",
    "\n",
    "![](advancedquery.png)\n",
    "\n",
    "A query might be too specific, too broad, or use different vocabulary than our source documents, leading to poor retrieval results.\n",
    "\n",
    "**Query Transformation** is a set of powerful techniques designed to re-write, expand, or break down the original question to significantly improve retrieval accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36656d45",
   "metadata": {},
   "source": [
    "## Multi-Query Generation\n",
    "\n",
    "A single user query represents just one perspective. Distance-based similarity search might miss relevant documents that use synonyms or discuss related concepts.\n",
    "\n",
    "The Multi-Query approach tackles this by using an LLM to generate several different versions of the user’s question, effectively searching from multiple angles.\n",
    "\n",
    "![](multiquery.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c0f67a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspectives | ChatOpenAI(temperature=0) | StrOutputParser() | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cf832b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 1. How do LLM agents utilize task decomposition in their operations?\n",
      "2. 2. Can you explain the concept of task decomposition as applied to LLM agents?\n",
      "3. 3. In what ways do LLM agents benefit from task decomposition?\n",
      "4. 4. What role does task decomposition play in the functioning of LLM agents?\n",
      "5. 5. How is task decomposition integrated into the workflow of LLM agents?\n"
     ]
    }
   ],
   "source": [
    "question = \"What is task decomposition for LLM agents?\"\n",
    "generated_queries_list = generate_queries.invoke({\"question\": question})\n",
    "\n",
    "for i, q in enumerate(generated_queries_list):\n",
    "    print(f\"{i+1}. {q}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd79f27a",
   "metadata": {},
   "source": [
    "Now, we can retrieve documents for all of these queries and combine the results. A simple way to combine them is to take the unique set of all retrieved documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f02eb813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique documents retrieved: 1\n"
     ]
    }
   ],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    flattened_docs = [dumps(docs) for sublist in documents for doc in sublist]\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "\n",
    "docs = retrieval_chain.invoke({\"question\": question})\n",
    "\n",
    "print(f\"Total unique documents retrieved: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2821c274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition for LLM (Large Language Model) agents involves breaking down complicated tasks into smaller, manageable steps to enhance performance and aid in planning. This can be achieved through various methods:\\n\\n1. **Chain of Thought (CoT)**: A prompting technique where the model is encouraged to \"think step by step,\" allowing it to decompose hard tasks into simpler components.\\n2. **Tree of Thoughts**: An extension of CoT that decomposes a problem into multiple thought steps, generating several thoughts for each step, which creates a tree structure. This approach allows for exploring multiple reasoning possibilities and can utilize search processes like breadth-first search (BFS) or depth-first search (DFS), with evaluations made by a classifier or through majority vote.\\n3. **Direct Prompts**: Simple prompting types, such as asking for subgoals or outlining steps for a specific task.\\n4. **Task-Specific Instructions**: Providing specific instructions tailored to the task, such as outlining a story for writing.\\n5. **Human Inputs**: Involving human interaction to guide the decomposition process.\\n\\nAdditionally, an approach called LLM+P involves using an external planner, where the LLM translates a problem into Planning Domain Definition Language (PDDL), requests a classical planner to generate a plan, and then translates that plan back into natural language, effectively outsourcing the planning step.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain, \"question\": itemgetter(\"question\")} | prompt | llm | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc72b439",
   "metadata": {},
   "source": [
    "This answer is more robust because it’s based on a wider pool of relevant documents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f4e965",
   "metadata": {},
   "source": [
    "## RAG-Fusion\n",
    "\n",
    "RAG-Fusion improves on Multi-Query by not just fetching documents, but also re-ranking them using a technique called Reciprocal Rank Fusion (RRF).\n",
    "\n",
    "RRF intelligently combines results from multiple searches. It boosts the score of documents that appear consistently high across different result lists, pushing the most relevant content to the top.\n",
    "\n",
    "![](ragfusion.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8633ba76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    fused_scores = {}\n",
    "    \n",
    "    for docs in results:\n",
    "        for rank, doc in enumerate(docs):\n",
    "            doc_str = dumps(doc)\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "            \n",
    "    reranked_results = [\n",
    "        (loads(doc), score) for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "    return reranked_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6aa45be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total re-ranked documents retrieved: 5\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (4 queries):\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_rag_fusion \n",
    "    | ChatOpenAI(temperature=0)\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "# Build the new retrieval chain with RRF\n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "\n",
    "print(f\"Total re-ranked documents retrieved: {len(docs)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07479737",
   "metadata": {},
   "source": [
    "## Decomposition\n",
    "\n",
    "The Decomposition technique uses an LLM to break down a complex query into a set of simpler, self-contained sub-questions. We can then answer each one and synthesize a final answer.\n",
    "\n",
    "![](decomposition1.png)\n",
    "---\n",
    "![](decomposition2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "55c5bdd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1. What are the core functionalities of an LLM (Large Language Model) in an autonomous agent system?', '2. How does natural language understanding contribute to the efficiency of LLM-powered autonomous agents?', '3. What role does reinforcement learning play in the development of LLM-powered autonomous agent systems?']\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):\"\"\"\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_queries_decomposition = (\n",
    "    prompt_decomposition | llm | StrOutputParser() | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "question = \"What are the main components of an LLM-powered autonomous agent system?\"\n",
    "sub_questions = generate_queries_decomposition.invoke({\"question\": question})\n",
    "print(sub_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e25fbaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_rag = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "rag_results = []\n",
    "for sub_question in sub_questions:\n",
    "    retrieved_docs = retriever.get_relevant_documents(sub_question)\n",
    "    answer = (prompt_rag|llm|StrOutputParser()).invoke({\"context\": retrieved_docs, \"question\": sub_question})\n",
    "    rag_results.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2ac1a318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_qa_pairs(questions, answers):\n",
    "    formatted_string = \"\"\n",
    "    for i, (question, answer) in enumerate(zip(questions, answers), start=1):\n",
    "        formatted_string += f\"Question {i}: {question} \\n Answer {i}: {answer} \\n\\n\"\n",
    "    return formatted_string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e1a066cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An LLM-powered autonomous agent system is composed of several key components that work together to enhance its functionality and effectiveness. These components include:\\n\\n1. **Large Language Model (LLM)**: The LLM serves as the central processing unit, functioning like the “brain” of the agent. It performs tasks such as planning, where complex tasks are broken down into manageable subgoals, and reflection, allowing the agent to learn from past actions and improve its future performance.\\n\\n2. **Natural Language Understanding (NLU)**: This component enables effective communication between the LLM and external systems, such as memory and tools. Through NLU, the agent can comprehend and interpret natural language, allowing it to navigate complex tasks and self-reflect on previous actions for enhanced outcomes.\\n\\n3. **Memory Capabilities**: A critical aspect of the system, memory allows the agent to retain both short-term and long-term information. This retention is essential for informed decision-making, enabling the agent to reference past experiences when solving new challenges.\\n\\n4. **Reinforcement Learning (RL)**: This element facilitates the approach of learning through trial and error. Reinforcement learning allows the agent to refine its strategies and decision-making processes by assessing the outcomes of its actions, making iterative improvements to enhance overall effectiveness.\\n\\nIn summary, the main components of an LLM-powered autonomous agent system involve the integration of an LLM, natural language understanding, memory capabilities, and reinforcement learning to enable sophisticated problem-solving, efficient communication, and continuous performance enhancement.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = format_qa_pairs(sub_questions, rag_results)\n",
    "\n",
    "# Final synthesis prompt\n",
    "template = \"\"\"Here is a set of Q+A pairs:\n",
    "\n",
    "{context}\n",
    "\n",
    "Use these to synthesize an answer to the original question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"context\": context, \"question\": question})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732252f7",
   "metadata": {},
   "source": [
    "## Step-Back Prompting\n",
    "\n",
    "Sometimes, a user’s query is too specific, while our documents contain the more general, underlying information needed to answer it.\n",
    "\n",
    "![](stepback.png)\n",
    "\n",
    "The Step-Back technique uses an LLM to take a “step back” and form a more general question. We then retrieve context for both the specific and general questions, providing a richer context for the final answer.\n",
    "\n",
    "We can teach the LLM this pattern using few-shot examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "73d73082",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
    "        \"output\": \"what can the members of The Police do?\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Jan Sindel's was born in what country?\",\n",
    "        \"output\": \"what is Jan Sindel's personal history?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"ai\"), \"{output}\"\n",
    "])\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \n",
    "    \"You are an expert at world knowledge. Your task is to step back and paraphrase a question \"\n",
    "    \"to a more generic step-back question, which is easier to answer. Here are a few examples:\"),\n",
    "    few_shot_prompt,\n",
    "    (\"user\", \"{question}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ccebcd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Question: What is task decomposition for LLM agents?\n",
      "Step-Back Question: What are the steps involved in breaking down tasks for LLM agents?\n"
     ]
    }
   ],
   "source": [
    "# Define a chain to generate step-back questions using the prompt and an OpenAI model\n",
    "generate_queries_step_back = prompt | ChatOpenAI(temperature=0) | StrOutputParser()\n",
    "\n",
    "# Run the chain on a specific question\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "step_back_question = generate_queries_step_back.invoke({\"question\": question})\n",
    "\n",
    "# Output the original and generated step-back question\n",
    "print(f\"Original Question: {question}\")\n",
    "print(f\"Step-Back Question: {step_back_question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "69808319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition for LLM agents refers to the process of breaking down complex tasks into smaller, more manageable subgoals or steps that can be easily handled by the agent. This decomposition can be achieved through various methods, such as simple prompting, task-specific instructions, or human inputs.\\n\\nOne approach to task decomposition involves using simple prompts like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\" to guide the LLM in breaking down the task into smaller components. Another method is to provide task-specific instructions, such as asking the agent to \"Write a story outline\" for writing a novel. Additionally, human inputs can also be used to decompose tasks for LLM agents.\\n\\nIn some cases, a more advanced approach known as LLM+P involves relying on an external classical planner to perform long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. The LLM translates the problem into \"Problem PDDL\", requests a classical planner to generate a PDDL plan based on an existing \"Domain PDDL\", and then translates the PDDL plan back into natural language. This outsourcing of the planning step to an external tool is common in certain robotic setups.\\n\\nOverall, task decomposition for LLM agents is essential for enabling them to efficiently handle complex tasks by breaking them down into smaller, more manageable subgoals. This process allows the agent to plan ahead, reflect on past actions, learn from mistakes, and ultimately improve the quality of the final results.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Prompt for the final response\n",
    "response_prompt_template = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "# Normal Context\n",
    "{normal_context}\n",
    "\n",
    "# Step-Back Context\n",
    "{step_back_context}\n",
    "\n",
    "# Original Question: {question}\n",
    "# Answer:\"\"\"\n",
    "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
    "\n",
    "# The full chain\n",
    "chain = (\n",
    "    {\n",
    "        # Retrieve context using the normal question\n",
    "        \"normal_context\": RunnableLambda(lambda x: x[\"question\"]) | retriever,\n",
    "        # Retrieve context using the step-back question\n",
    "        \"step_back_context\": generate_queries_step_back | retriever,\n",
    "        # Pass on the original question\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "    }\n",
    "    | response_prompt\n",
    "    | ChatOpenAI(temperature=0)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f488355c",
   "metadata": {},
   "source": [
    "## HyDE\n",
    "\n",
    "**HyDE (Hypothetical Document Embeddings)** proposes a radical solution: First, have an LLM generate a hypothetical answer to the question. This fake document, while not factually correct, will be semantically rich and use the kind of language we expect to find in a real answer.\n",
    "\n",
    "We then embed this hypothetical document and use its embedding to perform the retrieval. The result is that we find real documents that are semantically very similar to an ideal answer.\n",
    "\n",
    "![](hyde.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d3487952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Task Decomposition for LLM Agents: An Overview**\n",
      "\n",
      "Task decomposition is a critical process in the effective utilization of Large Language Model (LLM) agents, which refers to the systematic breakdown of a complex task into smaller, manageable subtasks. This approach is essential in enhancing the efficiency and performance of LLM agents when tackling multifaceted problems that require sequential reasoning and problem-solving abilities.\n",
      "\n",
      "In the context of LLM agents, task decomposition often involves identifying key components of a larger objective, categorizing them into discrete actions or steps, and establishing relationships between these components. For instance, when faced with the task of generating a comprehensive report, the LLM agent can decompose this task into several smaller subtasks: conducting research, outlining the report structure, drafting individual sections, and finally, revising the content for clarity and coherence.\n",
      "\n",
      "The methodology of task decomposition includes various strategies such as hierarchical decomposition, where tasks are arranged in a tiered structure based on dependencies and priorities, and functional decomposition, which focuses on specific functionalities required to achieve the overarching goal. By employing these strategies, LLM agents can leverage their advanced natural language understanding and generation capabilities more effectively, resulting in improved accuracy and relevance in their outputs.\n",
      "\n",
      "Furthermore, task decomposition enhances the interpretability of the LLM’s processes, as it allows developers and users to trace how individual subtasks contribute to the final outcome. This transparency is vital in applications that demand accountability and trust, particularly in sensitive domains such as healthcare and law. Overall, task decomposition not only facilitates better task management for LLM agents but also aligns their functioning with human cognitive processes, making them more adept at addressing complex queries and tasks in real-world scenarios.\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"Please write a scientific paper passage to answer the question\n",
    "Question: {question}\n",
    "Passage:\"\"\"\n",
    "prompt_hyde = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_docs_for_retrieval = (\n",
    "    prompt_hyde | llm | StrOutputParser()\n",
    ")\n",
    "\n",
    "hypothetical_document = generate_docs_for_retrieval.invoke({\"question\": question})\n",
    "\n",
    "print(hypothetical_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dc0c0d59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition for LLM (large language model) agents refers to the process by which an agent breaks down large, complex tasks into smaller, more manageable subgoals. This allows the agent to handle intricate tasks more efficiently and effectively. By decomposing tasks, the LLM agent can focus on one subgoal at a time, making it easier to plan and execute actions toward achieving the overall objective.\\n\\nThis process also incorporates elements of self-reflection and refinement, where the agent can evaluate past actions and learn from mistakes, thus improving future performance. The ability to decompose tasks is essential for the agent to optimize its problem-solving capabilities and navigate real-world challenges where iterative improvement is often necessary.'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain = generate_docs_for_retrieval | retriever\n",
    "retriever_docs = retrieval_chain.invoke({\"question\": question})\n",
    "final_rag_chain.invoke({\"context\": retrieved_docs, \"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a4510d",
   "metadata": {},
   "source": [
    "## Routing & Query Construction\n",
    "\n",
    "We often have multiple data sources: documentation for different programming languages, internal wikis, public websites, or databases with structured metadata.\n",
    "\n",
    "![](routing.png)\n",
    "\n",
    "This is where our RAG system needs to evolve from a simple librarian into an intelligent switchboard operator. It needs the ability to first analyze an incoming query and then route it to the correct destination or construct a more precise, structured query for retrieval. This section dives into the techniques that make this possible.\n",
    "\n",
    "### Logical Routing\n",
    "\n",
    "Routing is a classification problem. Given a user’s question, we need to classify it into one of several predefined categories. While traditional ML models can do this, we can leverage the powerful reasoning engine we already have: the LLM itself.\n",
    "\n",
    "![](logical.png)\n",
    "\n",
    "By providing the LLM with a clear schema (a set of possible categories), we can ask it to make the classification decision for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bef785c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class RouteQuery(BaseModel):\n",
    "    datasource: Literal[\"python_docs\", \"js_docs\", \"golang_docs\"] = Field(\n",
    "        ..., \n",
    "        description=\"Given a user question, choose which datasource would be most relevant for answering their question.\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "096ffcf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asohi\\Downloads\\RAG Ecosystem\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1900: UserWarning: Received a Pydantic BaseModel V1 schema. This is not supported by method=\"json_schema\". Please use method=\"function_calling\" or specify schema via JSON Schema or Pydantic V2 BaseModel. Overriding to method=\"function_calling\".\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "structured_llm = llm.with_structured_output(RouteQuery)\n",
    "system = \"\"\"ou are an expert at routing a user question to the appropriate data source.\n",
    "Based on the programming language the question is referring to, route it to the relevant data source.\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\")\n",
    "    ]\n",
    ")\n",
    "router = prompt | structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "43c4b225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasource='python_docs'\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"Why doesn't the following code work:\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\"human\", \"speak in {language}\"])\n",
    "prompt.invoke(\"french\")\n",
    "\"\"\"\n",
    "\n",
    "# Invoke the router and check the result\n",
    "result = router.invoke({\"question\": question})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dc3c5865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chain for python_docs\n"
     ]
    }
   ],
   "source": [
    "def choose_route(result):\n",
    "    if \"python_docs\" in result.datasource.lower():\n",
    "        return \"chain for python_docs\"\n",
    "    elif \"js_docs\" in result.datasource.lower():\n",
    "        return \"chain for js_docs\"\n",
    "    else:\n",
    "        return \"chin for golang_docs\"\n",
    "\n",
    "full_chain = router | RunnableLambda(choose_route)\n",
    "\n",
    "final_destination = full_chain.invoke({\"question\": question})\n",
    "\n",
    "print(final_destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b12348",
   "metadata": {},
   "source": [
    "### Semantic Routing\n",
    "\n",
    "Logical routing works perfectly when you have clearly defined categories. But what if you want to route based on the style or domain of a question? For example, you might want to answer physics questions with a serious, academic tone and math questions with a step-by-step, pedagogical approach. This is where Semantic Routing comes in.\n",
    "\n",
    "![](semanticrouting.png)\n",
    "\n",
    "`Instead of classifying the query, we define multiple expert prompts.`\n",
    "\n",
    "We then embed the user’s query and each of our prompt templates, and use cosine similarity to find the prompt that is most semantically aligned with the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3f749638",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# A prompt for a physics expert\n",
    "physics_template = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\"\n",
    "\n",
    "# A prompt for a math expert\n",
    "math_template = \"\"\"You are a very good mathematician. You are great at answering math questions. \\\n",
    "You are so good because you are able to break down hard problems into their component parts, \\\n",
    "answer the component parts, and then put them together to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2f516b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utils.math import cosine_similarity\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "prompt_templates = [physics_template, math_template]\n",
    "prompt_embeddings = embeddings.embed_documents(prompt_templates)\n",
    "\n",
    "def prompt_router(input):\n",
    "    query_embedding = embeddings.embed_query(input[\"query\"])\n",
    "    print(\"Query Embedding: \"+ str(query_embedding))\n",
    "    similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]\n",
    "    print(cosine_similarity([query_embedding], prompt_embeddings))\n",
    "    print(\"Similarity: \" + str(similarity))\n",
    "    most_similar_index = similarity.argmax()\n",
    "    chosen_prompt = prompt_templates[most_similar_index]\n",
    "    print(f\"DEBUG: Using {'MATH' if most_similar_index == 1 else 'PHYSICS'} template.\")\n",
    "    return PromptTemplate.from_template(chosen_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0e315ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Embedding: [-0.01060719694942236, -0.016601404175162315, -0.004036366939544678, 0.0006725881830789149, -0.039719998836517334, -0.0138792023062706, -0.011069837026298046, -0.028589816763997078, -0.002549549099057913, -0.035643402487039566, 0.04722951725125313, -0.0029116154182702303, -0.002331638941541314, -0.00944053940474987, -0.0017248429358005524, 0.018746981397271156, 0.03481198847293854, -0.020490262657403946, 0.007898406125605106, -0.01298744697123766, -0.025653056800365448, 0.019484523683786392, 0.0027138199657201767, -0.008233652450144291, -0.012471167370676994, 0.00789170153439045, 0.007529634982347488, -0.01356407068669796, -0.012129216454923153, -0.010560262948274612, 0.01473743375390768, -0.011157001368701458, -0.019927049055695534, -0.02617604099214077, -0.028267979621887207, 0.021630100905895233, -0.009916589595377445, -0.01483130268752575, 0.002247827360406518, 0.016829371452331543, 0.010399344377219677, 0.019980687648057938, -0.008173308335244656, -0.0009663478704169393, -0.020959606394171715, -0.00454258918762207, -0.024821646511554718, -0.01642707549035549, -0.008984604850411415, -0.003781579900532961, 0.024540038779377937, 0.00836775079369545, -0.0024221555795520544, -0.012873463332653046, 0.010694361291825771, -0.006208763923496008, -0.023547708988189697, -0.002591454889625311, 0.028107061982154846, 0.000879183760844171, -0.00434479396790266, -0.020530492067337036, 0.002130491193383932, -0.007958750240504742, 0.013718284666538239, 0.0031781361903995275, -0.024647317826747894, 0.0025461968034505844, -0.008977899327874184, -0.012310248799622059, 0.02732929028570652, 0.03379284217953682, -0.012544921599328518, 0.0024389177560806274, 0.010848574340343475, -0.014187629334628582, -0.03030627779662609, -0.0038519816007465124, 0.0027657831087708473, 0.009132113307714462, 0.03153998404741287, -0.007087109610438347, -0.0008221918833442032, 0.02252856083214283, 0.005776295904070139, 0.00011366949183866382, -0.022863807156682014, 0.027463387697935104, -0.019967278465628624, 0.02046344242990017, -0.011170411482453346, -0.017902160063385963, 0.022300593554973602, 0.014147399924695492, -0.004696802701801062, -0.0016812608810141683, 0.0010903890943154693, 0.043930694460868835, -0.007985570468008518, -0.025706697255373, -0.012846643105149269, -0.004666630644351244, -0.030681753531098366, -0.010540148243308067, 0.012967332266271114, 0.004944885149598122, -0.01114359125494957, -0.009916589595377445, 0.026511289179325104, 0.008039209991693497, 0.009802605956792831, 0.00617859186604619, 0.010499918833374977, -0.04422571137547493, 0.005622082855552435, -0.005585205741226673, 0.021951936185359955, -0.0030356564093381166, -0.007576569449156523, -0.032183658331632614, 0.02291744574904442, 0.011693395674228668, 0.01792898029088974, -0.001019987277686596, 0.03285415098071098, 0.005182909779250622, -0.010600492358207703, -0.02344043180346489, 0.02352089062333107, -0.014469236135482788, 0.0011708481470122933, 0.02808024175465107, -0.022421281784772873, -0.001431502285413444, 0.021469181403517723, 0.013517136685550213, -0.020490262657403946, 0.008005685172975063, -0.016883010044693947, -0.012350479140877724, 0.006973125971853733, 0.024164563044905663, -0.008656063117086887, 0.00033021773560903966, -0.0034295711666345596, 0.03245185688138008, 0.008173308335244656, 0.0016896420856937766, -0.01582363247871399, -0.01316848024725914, 0.008776751346886158, -0.00265179923735559, -0.0019142571836709976, 0.008656063117086887, 0.014938581734895706, 0.0029166440945118666, -0.0032535665668547153, 0.0197929497808218, -6.621117063332349e-05, -0.016547763720154762, -0.022179903462529182, -0.0033608456142246723, 0.022394461557269096, -0.007911816239356995, 0.017231667414307594, 0.04478892311453819, -0.008810276165604591, -0.0015630865236744285, -0.00862924288958311, -0.0007216179510578513, -0.005813173018395901, 0.00824706256389618, -0.027812045067548752, 0.037815798074007034, -0.018733570352196693, 0.0201416052877903, 0.011136886663734913, 0.014254678972065449, -0.02295767515897751, -0.005176205188035965, -0.03438287600874901, 0.011659870855510235, 0.018492193892598152, 0.035321563482284546, -0.007817947305738926, 0.007777717430144548, 0.03395375981926918, -0.029716243967413902, 0.019323604181408882, -0.014858121983706951, 0.013228824362158775, 0.029340768232941628, -0.03223729878664017, 0.01378533337265253, -0.6402402520179749, -0.002549549099057913, -0.002703762613236904, -0.002428860403597355, -0.007985570468008518, 0.0033658742904663086, 0.02749020792543888, 0.003778227372094989, -0.007804537191987038, -0.014643564820289612, 0.019189506769180298, 0.009494178928434849, -0.01995386742055416, -0.004328031558543444, -0.017473043873906136, -0.013175184838473797, 0.009031538851559162, 0.0031580214854329824, -0.010218311101198196, -0.0069127813912928104, -0.022904036566615105, 0.012960627675056458, -0.02015501633286476, -0.0012538216542452574, 0.008542079478502274, 0.03760123997926712, 0.00882368627935648, -0.016480714082717896, -0.026605157181620598, -0.006560772657394409, -0.01888107880949974, 0.016520945355296135, -0.013235528953373432, -0.012223085388541222, 0.03923724219202995, -0.02276993729174137, -0.0005234035197645426, 0.034248776733875275, -0.0038955635391175747, 0.019806358963251114, 0.00030318848439492285, 0.0012554979184642434, 0.04114144295454025, -0.02046344242990017, 0.005035401321947575, -0.013584185391664505, 0.00012142206833232194, 0.002534463070333004, 0.004884540569037199, -0.017070747911930084, 0.005139328073710203, -0.0006059579318389297, -0.004552646540105343, 0.0323713943362236, -0.004033014643937349, -0.012739364989101887, 0.035562943667173386, -0.005803115665912628, 0.010345704853534698, 0.013235528953373432, 0.0021338434889912605, 0.01939065381884575, -0.01867993175983429, -0.029582146555185318, 0.002232741331681609, 0.0024841760750859976, -0.034999728202819824, 0.032907791435718536, 0.00950088445097208, 0.0052365493029356, 0.038727667182683945, 0.010439573787152767, -0.005531566217541695, 0.008977899327874184, 0.03381966054439545, 0.028428897261619568, 0.028777554631233215, 0.0009562904597260058, 0.010064098052680492, 0.00011105037992820144, -0.006118247285485268, 0.0022830283269286156, -0.03422195464372635, -0.018854260444641113, 0.026940403506159782, 0.008729817345738411, -0.006862494628876448, -0.007717372849583626, 0.001719814259558916, 0.002148929750546813, 0.005809820722788572, 0.015260417945683002, 8.33925514598377e-05, -0.033766020089387894, -0.02145577222108841, 0.008421390317380428, -0.005139328073710203, -0.01473743375390768, -0.0032787101808935404, -0.004361556377261877, -0.02791932411491871, -0.029045751318335533, 0.002998779295012355, 0.0016720416024327278, -0.004854368511587381, -0.012638790532946587, -0.019524753093719482, 0.00041591509943827987, 0.044118430465459824, -0.04205331206321716, -0.0026233033277094364, -0.008495144546031952, -0.01717802695930004, 0.013932841829955578, -0.021147346124053, -0.027235420420765877, 0.024043874815106392, 0.011029607616364956, 0.025492139160633087, -0.0138792023062706, 0.039049506187438965, 0.02653810754418373, 0.01271254476159811, -0.006007615942507982, 0.001492684823460877, -0.0006562449270859361, 0.006037788465619087, -0.027114732190966606, -0.0632408857345581, -0.015086089260876179, -0.0016200784593820572, 0.02589443512260914, 0.027624307200312614, -0.0023366676177829504, 0.016883010044693947, 0.015005630441009998, 0.0007077890331856906, -0.03280051052570343, 0.0006407397449947894, -0.004063186701387167, -0.015314057469367981, 0.0034262186381965876, 0.03379284217953682, 0.013724989257752895, -0.01995386742055416, -0.04025639221072197, -0.0003572469868231565, -0.023225873708724976, -0.009789195843040943, -0.002698733936995268, -0.0019963926170021296, -0.014066940173506737, -0.022823577746748924, -0.0001635374064790085, 0.0038117519579827785, -0.029179850593209267, -0.006507133133709431, -0.04347475990653038, -0.024486400187015533, -0.01178056001663208, 0.012042052112519741, 0.017942389473319054, -0.0011130181374028325, -0.0025009384844452143, -0.00912540778517723, -0.02276993729174137, -0.016212517395615578, 0.019524753093719482, 0.0076905530877411366, -0.0057729436084628105, 0.00033608454396016896, 0.013631120324134827, -8.045914728427306e-05, 0.02530440129339695, 0.0065171909518539906, -0.003080914728343487, -0.014308317564427853, 0.01532746758311987, -0.024888696148991585, -0.005068926140666008, 0.028053421527147293, -0.007878291420638561, -0.017674192786216736, 0.017553502693772316, 0.0028529472183436155, 0.009681916795670986, 0.016869600862264633, -0.004851015750318766, -0.01951134204864502, -0.022622428834438324, -0.0012102395994588733, 0.01590409129858017, -0.02177760936319828, -0.004602933768182993, -0.028643455356359482, -0.015300647355616093, 0.019109046086668968, 0.012457757256925106, -0.009909885004162788, 0.042026493698358536, 0.008454915136098862, -0.02502279356122017, 0.016762321814894676, 0.0038117519579827785, 0.010553557425737381, 0.0030742099042981863, -0.001555543509311974, -0.01896153762936592, 0.005018639378249645, 0.004535884130746126, 0.004462129902094603, -0.02744997851550579, 0.0011180468136444688, 0.0016192402690649033, 0.02038298361003399, 0.03792307525873184, -0.0011792293516919017, 0.007073699962347746, 0.006751863285899162, -0.004897950682789087, 0.01338974293321371, 0.0034798579290509224, -0.003493267809972167, -0.014294908381998539, -0.0024188030511140823, -0.0022947618272155523, 0.03478517010807991, 0.03162044286727905, 0.00793193094432354, -0.022595610469579697, 0.017701011151075363, 0.017473043873906136, 0.02086573839187622, 0.025290992110967636, 0.002148929750546813, 0.018277635797858238, 0.006718338467180729, 0.011619641445577145, 0.03853993117809296, -0.022823577746748924, -0.025612827390432358, 0.02295767515897751, 0.008166602812707424, 0.011666576378047466, 0.006124952342361212, 0.007308372296392918, 0.03526792675256729, -0.017915569245815277, -0.01271254476159811, -0.0015873918309807777, -0.02396341599524021, 0.014817892573773861, -0.00624564103782177, 0.008917555212974548, 0.027704766020178795, -0.024446170777082443, -0.0007492757868021727, -0.006091427989304066, -0.005176205188035965, 0.018492193892598152, 0.004401785787194967, 0.01196159329265356, -0.004509064368903637, -0.009769081138074398, 0.010895509272813797, 0.005920452065765858, -0.006366329733282328, 0.015850450843572617, 0.008756636641919613, 0.021308263763785362, -0.0014440739760175347, -0.009950114414095879, 0.007925225421786308, -0.011948183178901672, 0.03105722926557064, 0.014750842936336994, 0.006158477161079645, 0.0005472897901199758, 0.014026710763573647, -0.005427639931440353, -0.020530492067337036, -0.031271789222955704, -0.002108700107783079, 0.014107170514762402, -0.016400255262851715, -0.025277581065893173, -0.006309337913990021, 0.030467195436358452, -0.006805502809584141, 0.022153085097670555, 0.006973125971853733, 0.013356218114495277, 0.009393605403602123, -0.008072733879089355, -0.022314002737402916, -0.017513273283839226, 0.007449175696820021, -0.01931019499897957, -0.0030909720808267593, -0.00412353128194809, -0.028428897261619568, -0.003206632100045681, 0.01302767638117075, -0.008951080031692982, 0.010459688492119312, 0.021361902356147766, 3.8370002584997565e-05, -0.021147346124053, 0.008220242336392403, -0.002903234213590622, -0.022381052374839783, -0.015582254156470299, 0.0013150041922926903, 0.00041821991908364, -0.004190580453723669, -0.007864881306886673, -0.018948128446936607, -0.027865683659911156, 0.027972962707281113, 0.025492139160633087, 0.008937669917941093, -0.01245105266571045, -0.01669527217745781, -0.023333152756094933, 0.09263529628515244, 0.022555381059646606, -0.003942498005926609, 0.022139674052596092, -0.01951134204864502, -0.00481749139726162, -0.03907632455229759, 0.001514475792646408, 0.004706860054284334, 0.0048610735684633255, 0.04052459076046944, -0.014750842936336994, -0.006765272933989763, 0.0028697093948721886, 0.018451964482665062, -0.009071768261492252, 0.019739311188459396, -0.003939145710319281, 0.006818912457674742, -0.006316042970865965, -0.016883010044693947, -0.009071768261492252, -0.0025847500655800104, 0.018076488748192787, 0.015381106175482273, -0.0037078256718814373, -0.012679019942879677, -0.0013845678186044097, 0.0062489937990903854, -0.021147346124053, -0.014630154706537724, -0.014549694955348969, 0.005112508311867714, -0.0028797669801861048, -0.004368260968476534, -0.0004823358030989766, -0.0042878021486103535, 0.0023819259367883205, 0.014093760401010513, -0.011619641445577145, 0.015635894611477852, -0.00506557384505868, 0.01552861463278532, -0.007482700515538454, 0.0020768516696989536, -0.01780829019844532, 0.014160809107124805, 0.02121439389884472, -0.015233597718179226, -0.015930911526083946, 0.023346561938524246, 0.019605211913585663, -0.017285306006669998, 0.00012330783647485077, 0.014066940173506737, 0.012665610760450363, 0.016199108213186264, -0.0009294707560911775, 0.010064098052680492, -0.025197122246026993, -0.018988357856869698, 0.00048526920727454126, 0.017513273283839226, -0.004971704445779324, -0.0039324406534433365, -0.03843265026807785, 0.002184130484238267, -0.023091774433851242, -0.021402131766080856, 0.010412754490971565, 0.011981707997620106, -0.03280051052570343, -0.031164508312940598, 0.021308263763785362, 0.011277690529823303, -0.008066029287874699, 0.005692484322935343, -0.013999891467392445, 0.00490800803527236, 0.00441184313967824, 0.003208308480679989, -0.025840794667601585, 0.02236764319241047, -0.02193852700293064, -0.007771012373268604, 0.013771923258900642, -0.0048744832165539265, -0.007610094267874956, 0.0009638335322961211, -0.0007480186177417636, 0.015769992023706436, 0.0014155780663713813, 0.02090596780180931, 0.0017416052287444472, -0.00022105312382336706, 0.00033231303677894175, 0.024768006056547165, 0.007757602725178003, 0.011934773065149784, -0.00019067141693085432, -0.004629753530025482, 0.004495654720813036, 0.005025343969464302, 0.0028646807186305523, 0.010486508719623089, 0.014294908381998539, 0.020289113745093346, 0.027235420420765877, -0.006215468980371952, -0.020409803837537766, -0.0009562904597260058, -0.04883870109915733, 0.009272916242480278, -0.02165691927075386, -0.004783966578543186, 0.02372203767299652, 0.03261277452111244, 0.012799709104001522, -0.04580807313323021, -0.023789087310433388, -0.03733304515480995, -0.020101375877857208, -0.0002560444700066, 0.023789087310433388, 0.010023868642747402, -0.0009898151038214564, -0.0006851599318906665, -0.01433513779193163, -0.020892558619379997, -0.007992275059223175, -0.013228824362158775, 0.00667475676164031, -0.012283429503440857, -0.016199108213186264, -0.03414149582386017, -0.0028495946899056435, -0.007308372296392918, 0.026471057906746864, -0.008843800984323025, -0.005531566217541695, -0.0038151044864207506, -0.005920452065765858, -0.0005058030365034938, -0.03674301132559776, -0.016748912632465363, -0.03550930321216583, -0.039129965007305145, -0.003674300853163004, 0.0031345542520284653, 0.014656974002718925, -0.014361957088112831, 0.005776295904070139, -0.00039915277739055455, -0.024741187691688538, -0.0038285143673419952, -0.02399023436009884, -0.03312234953045845, -0.006111542694270611, 0.02344043180346489, 0.02189829759299755, 0.02403046377003193, 0.015676124021410942, 0.027275649830698967, 0.019095636904239655, -0.004787319339811802, -0.021428951993584633, 0.01374510396271944, -0.007134044077247381, -0.01294721756130457, 0.011552591808140278, -0.0012563359923660755, 0.0009621572680771351, 0.031244967132806778, -0.015796812251210213, -0.0002537396503612399, 0.01780829019844532, 0.012337069027125835, 0.009789195843040943, -0.029796702787280083, -0.03403421863913536, -0.005243254359811544, -0.013114840723574162, 0.00667475676164031, -0.003888858715072274, -0.029099389910697937, -0.02899211272597313, 0.017902160063385963, 0.016199108213186264, -0.014268088154494762, -0.014724023640155792, 0.027865683659911156, 0.00801238976418972, 0.0308158528059721, 0.015045859850943089, 0.0006625307723879814, -0.00045886856969445944, -0.0029971031472086906, 0.011941478587687016, -0.0013367951614782214, 0.028804374858736992, 0.004106768872588873, 0.005343828350305557, -0.004837606102228165, -0.004106768872588873, -0.0012420880375429988, 0.0006864171009510756, 0.006503780838102102, 0.006212116684764624, 0.018063077703118324, -0.05465858057141304, -0.005803115665912628, -0.027704766020178795, -0.021884886547923088, -0.016708683222532272, -0.0036474813241511583, 0.004328031558543444, -0.025948073714971542, 0.02522394247353077, -0.029689423739910126, 0.009507589042186737, 0.008200127631425858, 0.018505603075027466, 0.010271950624883175, 0.012719250284135342, 0.03422195464372635, 0.03261277452111244, 0.024861875921487808, -0.020610950887203217, 0.01768760196864605, -0.01424126885831356, -0.008977899327874184, 0.029716243967413902, -0.001350205042399466, -0.011431903578341007, -0.009299736469984055, 0.010794935747981071, -0.006316042970865965, -0.007013355381786823, -0.02593466453254223, 0.010526738129556179, -0.012497987598180771, 0.010325590148568153, -0.0036877107340842485, -0.012068871408700943, -0.016829371452331543, 0.01178056001663208, -0.012256610207259655, 0.0028680332470685244, -0.02585420571267605, -0.021066885441541672, -0.02696722373366356, -0.0021321673411875963, -0.005933862179517746, 0.015622483566403389, 0.016802551224827766, -0.004153703339397907, 0.005011934321373701, -0.02074505016207695, -0.007589979562908411, 0.01378533337265253, 0.0013225472066551447, 0.041946034878492355, 0.0071809785440564156, -0.0055550336837768555, 0.000652473361697048, 0.004559351596981287, 0.013718284666538239, -0.033175986260175705, -0.015233597718179226, 0.026953814551234245, -0.02307836525142193, -0.013644530437886715, 0.002391983289271593, -0.027101321145892143, 0.016413666307926178, -0.001367805409245193, -0.007294962648302317, -0.013684759847819805, 0.027101321145892143, 0.00927962176501751, 0.008334226906299591, 0.020128196105360985, -0.011619641445577145, 0.016158878803253174, -0.0033859889954328537, -0.02177760936319828, -0.02281016856431961, 0.0010593787301331758, 0.013671349734067917, -0.024285251274704933, -0.03381966054439545, -0.025840794667601585, 0.0008716407464817166, 0.005216434597969055, -0.006748510990291834, -0.03164726495742798, -0.0007748383213765919, 0.03974681720137596, -0.015381106175482273, -0.0055248611606657505, 0.017392585054039955, 0.024365711957216263, -0.006577535066753626, 0.010895509272813797, 0.026471057906746864, -0.017285306006669998, 0.010144556872546673, -0.00346980057656765, -0.006222174037247896, -0.038888588547706604, 0.001531238085590303, 0.020208654925227165, -0.00036437096423469484, 0.009058359079062939, 0.013463497161865234, 0.016547763720154762, 0.004666630644351244, -0.011894543655216694, 0.005089040845632553, 0.01821058616042137, -0.008139783516526222, 0.02248833142220974, 0.007274847477674484, -0.02593466453254223, -0.015542024746537209, -0.0062054116278886795, -0.008032504469156265, 0.0026417418848723173, -0.017861930653452873, -0.006208763923496008, -0.016869600862264633, 0.02447298914194107, 0.005139328073710203, -0.016762321814894676, -0.025036204606294632, -0.014442416839301586, -0.03406103700399399, -0.0027087912894785404, 0.011834199540317059, -0.010526738129556179, -0.0027892503421753645, 0.027034273371100426, -0.01995386742055416, 0.00237019220367074, -0.0023618112318217754, 0.008361046202480793, 0.011177116073668003, -0.02331974171102047, -0.0057092467322945595, -0.012913692742586136, -0.023145414888858795, 0.039049506187438965, 0.012665610760450363, -0.013463497161865234, -0.01689642108976841, 0.020892558619379997, 0.010452983900904655, 0.018653111532330513, 0.019927049055695534, 0.021442363038659096, 0.00018040448776446283, 0.008977899327874184, -0.012048756703734398, 0.025653056800365448, 0.006091427989304066, 0.009956819005310535, 0.004224105272442102, -0.007737488020211458, -0.011633051559329033, 0.015689533203840256, 0.010298770852386951, -0.015421336516737938, -0.005152737721800804, -0.014120579697191715, 0.019283374771475792, -0.004465482663363218, 0.012156035751104355, 0.043930694460868835, -0.015112909488379955, -0.008394571021199226, 0.010359114967286587, 0.0020165073219686747, 0.003171431366354227, 0.015314057469367981, -0.002262913389131427, -0.04347475990653038, -0.009983639232814312, -0.006959715858101845, 0.02153623104095459, -0.009346670471131802, 0.014160809107124805, 0.013255644589662552, -0.009038244374096394, 0.0028931768611073494, -0.029421227052807808, -0.013476907275617123, 0.010882099159061909, 0.008233652450144291, 0.006953011266887188, -0.009326555766165257, -0.007603389210999012, 0.029260309413075447, 0.0169366504997015, -0.023225873708724976, -0.007569864392280579, -0.0009923294419422746, -0.020476851612329483, 0.012880167923867702, -0.0073016672395169735, -0.027651125565171242, 0.0174864549189806, -0.020678000524640083, 0.009286326356232166, -0.03253231570124626, 0.00801238976418972, -0.028536176308989525, -0.0047973766922950745, -0.04240196943283081, 0.010875394567847252, -0.01259185653179884, 0.000613920041359961, 0.014026710763573647, 0.0015781725523993373, -0.00037505695945583284, 0.002485852222889662, -0.005226491950452328, -0.0038352191913872957, -0.014361957088112831, 0.018773799762129784, 0.0008234490524046123, -0.004653220530599356, -0.010520033538341522, 0.006785388104617596, -6.374920485541224e-05, 0.0006679785437881947, -0.0031613740138709545, 0.2101592868566513, -0.0052767787128686905, 0.003624014090746641, 0.044198889285326004, -0.03719894587993622, 0.01821058616042137, 0.015729762613773346, -0.015796812251210213, 0.006124952342361212, 0.03529474511742592, -0.005534918513149023, -0.0017701011383906007, -0.0053706481121480465, 0.0006692357128486037, -0.00789170153439045, -0.0028076888993382454, -0.03430241718888283, 0.00624564103782177, -0.0033021774142980576, -0.019055407494306564, -0.017982618883252144, 0.012638790532946587, -0.017553502693772316, -0.01756691373884678, 0.027463387697935104, -0.0031630501616746187, -0.004777261521667242, -0.029850343242287636, -0.00445207254961133, 0.006497075781226158, 0.013577480800449848, -0.022716298699378967, -0.008495144546031952, 0.016869600862264633, 0.008622538298368454, 0.013946251943707466, 0.008857211098074913, -0.0065440102480351925, 0.019725900143384933, 0.045593515038490295, -0.018733570352196693, -0.00192096212413162, -0.017110977321863174, 0.02700745314359665, -0.011659870855510235, -0.0019326957408338785, -0.00114821910392493, -0.0070401751436293125, 0.01931019499897957, 0.02757066674530506, -0.01701710931956768, -0.020731639117002487, 0.04395751282572746, 0.046076271682977676, -0.00490800803527236, 0.0025478729512542486, 0.0011716863373294473, -0.0016644985880702734, 0.00027553067775443196, 0.003171431366354227, -0.007583274506032467, 0.04704178124666214, -0.00649372348561883, 0.0406855084002018, 0.013637824915349483, 0.015381106175482273, -0.028348438441753387, 0.0030155417043715715, 0.011492247693240643, -0.01552861463278532, 0.020235475152730942, -0.03344418480992317, -0.0051225656643509865, -0.02990398183465004, -0.0061283051036298275, -0.018331274390220642, 0.02923348918557167, 0.01821058616042137, 0.025800565257668495, 0.03320280835032463, -0.01813012734055519, -0.01776806078851223, -0.002842889865860343, -0.005035401321947575, -0.012021937407553196, -0.02867027558386326, -0.0045492942444980145, -0.0006059579318389297, -0.015635894611477852, -0.0019226382719352841, 0.0062188212759792805, 0.0037681700196117163, -0.01920291595160961, -0.006259051151573658, 0.017070747911930084, -0.015582254156470299, -0.007415651343762875, -0.01669527217745781, -0.029287129640579224, 0.005739418789744377, -0.0197929497808218, -0.014496056362986565, 0.015045859850943089, -0.010365819558501244, 0.00184217921923846, -0.006476961076259613, -0.001163305132649839, 0.0018069782527163625, 0.01955157145857811, -0.035080187022686005, -0.012330364435911179, -0.004183875396847725, 0.006785388104617596, -0.0059137470088899136, -0.009702031500637531, 0.02161668986082077, -0.011183821596205235, 0.008072733879089355, 0.0039056208916008472, -0.032264117151498795, 0.002531110541895032, -0.025518959388136864, 0.0037949897814542055, -0.007978864945471287, -0.013986481353640556, -0.02169714868068695, 0.007898406125605106, -0.022139674052596092, 0.018867669627070427, -0.0399881936609745, 0.025653056800365448, -0.031244967132806778, -0.0037312929052859545, 0.008937669917941093, -0.006024378351867199, 0.018586061894893646, -0.0023618112318217754, -0.026484468951821327, -0.017781471833586693, -0.013081315904855728, -0.014254678972065449, 0.02074505016207695, -0.012028641998767853, -0.0012596885208040476, 0.012497987598180771, -0.03390011936426163, 0.019766129553318024, -0.008387865498661995, -0.004539236892014742, -0.01967226155102253, -0.03733304515480995, -0.020691409707069397, -0.019068816676735878, -0.007006650324910879, 0.010479803197085857, -0.0014239592710509896, -0.03505336865782738, -0.02773158624768257, -0.018653111532330513, 0.01259185653179884, -0.033739201724529266, 0.007402241230010986, 0.036823470145463943, -0.01967226155102253, -0.0019896875601261854, 0.006849084515124559, -0.17110978066921234, -0.007261437829583883, 0.014536285772919655, -0.015649303793907166, 0.03360510244965553, 0.0002860071253962815, 0.0034178374335169792, 0.007261437829583883, -0.015877271071076393, 0.026605157181620598, -0.02352089062333107, -0.00232996279373765, -0.022461511194705963, -0.018733570352196693, 0.017794881016016006, 0.005538271274417639, 0.011592822149395943, -0.0011591145303100348, 0.02118757553398609, 0.00718768360093236, 0.03703802824020386, -0.011076542548835278, 0.01689642108976841, -0.006473608780652285, 0.006440083961933851, -0.002185806864872575, -0.012028641998767853, 0.036984387785196304, 0.015179959125816822, -0.003268652828410268, 0.00163348822388798, -0.026859944686293602, 0.013450087048113346, 0.01847878284752369, 0.024245021864771843, -0.0068993717432022095, 0.007703963201493025, -0.0028512710705399513, 0.009775785729289055, -0.000388257292797789, 0.038888588547706604, 0.008609128184616566, 0.0016242689453065395, 0.0004366584762465209, 0.00971544161438942, 0.024969154968857765, 0.017955798655748367, 0.004016252234578133, 0.016869600862264633, -0.01851901412010193, 0.017124388366937637, -0.023614758625626564, 0.0054477546364068985, 0.029608964920043945, 0.0231990534812212, 0.0045694089494645596, -0.007201093714684248, 0.01764737255871296, -0.012082281522452831, -0.027034273371100426, -0.03296142816543579, -0.0014893322950229049, -0.033175986260175705, -0.0028345086611807346, -0.018304456025362015, 0.00220592156983912, 0.0023014668840914965, -0.014884942211210728, -0.01642707549035549, -0.00608807522803545, -0.012605265714228153, -0.010043983347713947, -0.012370593845844269, 3.258176366216503e-05, -0.004173818044364452, 0.01051332801580429, -0.01544815581291914, 0.021884886547923088, -0.00975567102432251, -0.01017137710005045, 0.0009688622085377574, 0.017459634691476822, -0.009078473784029484, -0.014710613526403904, 0.005638845264911652, 0.002817746251821518, -0.028589816763997078, -0.011358149349689484, 0.0030725335236638784, 0.00346980057656765, 0.0192431453615427, -0.028053421527147293, 0.014080350287258625, -0.003838571719825268, -0.00011052656191168353, 0.023829316720366478, 0.02141554281115532, 0.0021958642173558474, 0.014214448630809784, 0.021442363038659096, 0.011606231331825256, 0.02396341599524021, -0.029608964920043945, -0.0013166803400963545, 0.04167783632874489, 0.018814031034708023, -0.009326555766165257, 0.011076542548835278, 0.04696132242679596, -0.011545887216925621, -0.011170411482453346, -0.02749020792543888, 0.030440377071499825, 0.007194388657808304, 0.003002131823450327, 0.041543737053871155, 0.0249021053314209, -0.005464517045766115, 0.03711848706007004, -0.022059215232729912, 0.0203427542001009, 0.006416616961359978, -0.032585952430963516, -0.001962867798283696, -0.006198706571012735, -0.021951936185359955, -0.0989111065864563, -0.02323928289115429, 0.0018924660980701447, 0.009078473784029484, 0.015019040554761887, 0.006118247285485268, 0.007925225421786308, -0.009078473784029484, -0.012786298990249634, 0.040122292935848236, -0.004931475035846233, -0.029957622289657593, -0.010191491805016994, 0.007864881306886673, 0.018626291304826736, -0.0167355015873909, 0.013336103409528732, 0.008803571574389935, -0.01796920970082283, 0.020932788029313087, -0.016587993130087852, 0.011297805234789848, -0.008756636641919613, -0.005025343969464302, 0.00598079664632678, 0.004146998282521963, -0.02692699432373047, 0.022743118926882744, 0.0277584046125412, 0.009970229119062424, -0.019967278465628624, -0.0003962193732149899, -0.03030627779662609, -0.02443275973200798, -0.007623503915965557, 0.011331330053508282, -0.012933807447552681, 0.011619641445577145, 0.03344418480992317, -0.03223729878664017, -0.00583999278023839, 0.022072626277804375, 0.0334710031747818, -0.0037849321961402893, 0.0042442199774086475, -0.019135866314172745, 0.000983948353677988, 0.015381106175482273, 0.016681862995028496, 0.0005066411686129868, -0.017580322921276093, -0.02471436746418476, -0.05476585775613785, 0.0006114057032391429, 0.021831247955560684, -0.04977739229798317, 0.010734590701758862, -0.0015169901307672262, -0.007140749134123325, -0.009567933157086372, -0.004666630644351244, 0.022059215232729912, -0.010023868642747402, 0.033739201724529266, 0.02558600902557373, -0.0031948985997587442, -0.02388295717537403, -0.015609074383974075, 0.018291044980287552, -0.029769884422421455, -0.012202970683574677, 0.013329398818314075, -0.016722092404961586, -0.00412353128194809, -0.008897440508008003, 0.013758514076471329, -0.029421227052807808, 0.03022581897675991, 0.01800943911075592, -0.012497987598180771, -0.010640721768140793, -0.024258432909846306, 0.018036257475614548, -0.010962558910250664, 0.01837150566279888, 0.012974036857485771, -0.002930053975433111, -0.004824196454137564, 0.00768384849652648, -0.03518746793270111, 0.009031538851559162, 0.016829371452331543, 0.014670384116470814, -0.03800353780388832, -0.012082281522452831, 0.017660781741142273, 0.008857211098074913, -0.004877835512161255, 0.04374295473098755, -0.008917555212974548, -0.021040067076683044, -0.006580887362360954, -0.0457007959485054, 0.023534299805760384, -0.001738252816721797, -0.0026417418848723173, -0.00396596547216177, -0.010238425806164742, -0.013959662057459354, -0.006373034790158272, -0.0023601348511874676, 0.03352464362978935, -0.0038452765438705683, 0.00997693371027708, -0.00039538127020932734, -0.004455425310879946, -0.016118649393320084, -0.024915514513850212, 0.009802605956792831, -0.006704928819090128, -0.00441184313967824, -0.005075631197541952, -0.008709702640771866, 0.023386791348457336, 0.008749932050704956, 0.015166549012064934, 0.018116718158125877, -0.012102396227419376, -0.017902160063385963, 0.010634017176926136, -0.0330687090754509, -0.00667475676164031, 0.01483130268752575, -0.04612990841269493, 0.006349567323923111, 0.030252639204263687, -0.01199511718004942, -0.02201898582279682, -0.001857265247963369, 0.008300702087581158, 0.022756528109312057, 0.025679877027869225, -0.010996082797646523, -0.026994043961167336, -0.0031881935428828, 0.002108700107783079, -0.010680951178073883, 0.0209461972117424, 0.0023064955603331327, -1.7417100025340915e-05, 0.00963498279452324, 0.0081934230402112, 0.03794989734888077, 0.0285629965364933, -0.015381106175482273, 0.001366967335343361, -0.03567022085189819, -0.023775678128004074, -0.01007750816643238, 0.015381106175482273, 0.00030612191767431796, -0.01912245713174343, 0.01536769699305296, 0.016842780634760857, 0.04084642603993416, -0.002594807418063283, -0.0016418694285675883, 0.009165637195110321, 0.01876039057970047, 0.01003727875649929, 0.03191545978188515, -0.057769667357206345, -0.014469236135482788, -0.00043372507207095623, 0.02542508952319622, 0.02974306419491768, -0.005162795074284077, -0.03601887822151184, -0.01231695432215929, -0.012015231885015965, -0.009923294186592102, 0.007777717430144548, 0.006785388104617596, 0.0029702833853662014, 0.0012077252613380551, -0.012960627675056458, 0.03719894587993622, 0.012457757256925106, -0.0009319850942119956, -0.004462129902094603, 0.01433513779193163, 0.021040067076683044, -0.00035892322193831205, 0.03569703921675682, 0.009567933157086372, -0.007522929925471544, -0.018948128446936607, 0.012008527293801308, -0.0016452218405902386, -0.0006072151008993387, 0.00599755859002471, 0.015676124021410942, 0.017794881016016006, -0.012779594399034977, -0.02641741931438446, -0.011036313138902187, -0.0023584587033838034, 0.01900176890194416, -0.002244474831968546, -0.003556964686140418, 0.004995171912014484, 0.036984387785196304, 0.020718229934573174, 0.001061893068253994, 0.004844311159104109, 0.021402131766080856, -0.013852383010089397, 0.012900282628834248, -0.016360025852918625, -0.02388295717537403, -0.001594934961758554, -0.0020952902268618345, 0.00912540778517723, 0.012390708550810814, 0.01548838522285223, -0.0015580578474327922, -0.0015672771260142326, 0.00824706256389618, 0.007462585810571909, -0.03191545978188515, 0.01391943171620369, -0.008059324696660042, -0.011545887216925621, -0.013061201199889183, -0.019068816676735878, 0.0022981143556535244, -0.0353752039372921, 0.0115995267406106, 0.0057092467322945595, 0.016561174765229225, -0.0059137470088899136, 0.08099553734064102, 0.01082845963537693, 0.003781579900532961, 0.00944053940474987, -0.013584185391664505, -0.0008942698477767408, 0.029206668958067894, -0.01013114769011736, -0.030359918251633644, -0.03129860758781433, 0.008206833153963089, -0.022515151649713516, 0.014603334479033947, -0.025250760838389397, -0.00583999278023839, 0.00470015499740839, 0.0046599255874753, 0.03288096934556961, -0.00963498279452324, 0.0057997633703053, 0.00460963835939765, -0.013657939620316029, 0.032183658331632614, 0.01155929733067751, -0.02582738548517227, -0.006242288742214441, -0.0005129270721226931, 0.02669902704656124, 0.013986481353640556, -0.027865683659911156, 0.013161774724721909, 0.026269910857081413, -0.019283374771475792, -0.017982618883252144, 0.013624414801597595, -0.02423161268234253, -0.005182909779250622, 0.0006063769687898457, 0.01302767638117075, 0.014361957088112831, 0.006775330286473036, 0.03923724219202995, -0.020932788029313087, -0.040712326765060425, -0.004086654167622328, -0.016829371452331543, 0.013999891467392445, 0.0017566913738846779, -0.010982673615217209]\n",
      "[[0.75292479 0.71735783]]\n",
      "Similarity: [0.75292479 0.71735783]\n",
      "DEBUG: Using PHYSICS template.\n",
      "A black hole is a region in space where the gravitational pull is so strong that nothing, not even light, can escape from it. This happens when a massive star collapses in on itself, leaving behind a small, incredibly dense core. While we can't directly see a black hole, we can observe its effects on surrounding matter and light.\n"
     ]
    }
   ],
   "source": [
    "# The final chain that combines the router with the LLM\n",
    "chain = (\n",
    "    {\"query\": RunnablePassthrough()}\n",
    "    | RunnableLambda(prompt_router)  # Dynamically select the prompt\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Ask a physics question\n",
    "print(chain.invoke(\"What's a black hole\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50dd391",
   "metadata": {},
   "source": [
    "### Query Structuring\n",
    "\n",
    "So far, we’ve focused on retrieving from unstructured text. But most real-world data is semi-structured; it contains valuable metadata like dates, authors, view counts, or categories. A simple vector search can’t leverage this information.\n",
    "\n",
    "`Query Structuring is the technique of converting a natural language question into a structured query that can use these metadata filters for highly precise retrieval.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d02626",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "\n",
    "docs = YoutubeLoader.from_youtube_url(\n",
    "    \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\", add_video_info=True\n",
    ").load()\n",
    "\n",
    "\n",
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9547ae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from typing import Optional\n",
    "\n",
    "class TutorialSearch(BaseModel):\n",
    "    \"\"\"A data model for searching over a database of tutorial videos.\"\"\"\n",
    "\n",
    "    # The main query for a similarity search over the video's transcript.\n",
    "    content_search: str = Field(..., description=\"Similarity search query applied to video transcripts.\")\n",
    "    \n",
    "    # A more succinct query for searching just the video's title.\n",
    "    title_search: str = Field(..., description=\"Alternate version of the content search query to apply to video titles.\")\n",
    "    \n",
    "    # Optional metadata filters\n",
    "    min_view_count: Optional[int] = Field(None, description=\"Minimum view count filter, inclusive.\")\n",
    "    max_view_count: Optional[int] = Field(None, description=\"Maximum view count filter, exclusive.\")\n",
    "    earliest_publish_date: Optional[datetime.date] = Field(None, description=\"Earliest publish date filter, inclusive.\")\n",
    "    latest_publish_date: Optional[datetime.date] = Field(None, description=\"Latest publish date filter, exclusive.\")\n",
    "    min_length_sec: Optional[int] = Field(None, description=\"Minimum video length in seconds, inclusive.\")\n",
    "    max_length_sec: Optional[int] = Field(None, description=\"Maximum video length in seconds, exclusive.\")\n",
    "\n",
    "    def pretty_print(self) -> None:\n",
    "        \"\"\"A helper function to print the populated fields of the model.\"\"\"\n",
    "        for field in self.__fields__:\n",
    "            if getattr(self, field) is not None:\n",
    "                print(f\"{field}: {getattr(self, field)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "46066f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asohi\\Downloads\\RAG Ecosystem\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1900: UserWarning: Received a Pydantic BaseModel V1 schema. This is not supported by method=\"json_schema\". Please use method=\"function_calling\" or specify schema via JSON Schema or Pydantic V2 BaseModel. Overriding to method=\"function_calling\".\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# System prompt for the query analyzer\n",
    "system = \"\"\"You are an expert at converting user questions into database queries. \\\n",
    "You have access to a database of tutorial videos about a software library for building LLM-powered applications. \\\n",
    "Given a question, return a database query optimized to retrieve the most relevant results.\n",
    "\n",
    "If there are acronyms or words you are not familiar with, do not try to rephrase them.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", \"{question}\")])\n",
    "structured_llm = llm.with_structured_output(TutorialSearch)\n",
    "\n",
    "# The final query analyzer chain\n",
    "query_analyzer = prompt | structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "56f83e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_search: rag from scratch\n",
      "title_search: rag from scratch\n"
     ]
    }
   ],
   "source": [
    "# Test 1: A simple query\n",
    "query_analyzer.invoke({\"question\": \"rag from scratch\"}).pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b30c7c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_search: chat langchain\n",
      "title_search: chat langchain\n",
      "earliest_publish_date: 2023-01-01\n",
      "latest_publish_date: 2024-01-01\n"
     ]
    }
   ],
   "source": [
    "query_analyzer.invoke(\n",
    "    {\"question\": \"videos on chat langchain published in 2023\"}\n",
    ").pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4670dde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_search: multi-modal models in an agent\n",
      "title_search: multi-modal models in an agent\n",
      "max_length_sec: 300\n"
     ]
    }
   ],
   "source": [
    "query_analyzer.invoke(\n",
    "    {\n",
    "        \"question\": \"how to use multi-modal models in an agent, only videos under 5 minutes\"\n",
    "    }\n",
    ").pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0732788",
   "metadata": {},
   "source": [
    "## Advanced Indexing Strategies\n",
    "So far, our approach to indexing has been straightforward: split documents into chunks and embed them. This works, but it has a fundamental limitation.\n",
    "\n",
    "Small, focused chunks are great for retrieval accuracy (they contain less noise), but they often lack the broader context needed for the LLM to generate a comprehensive answer.\n",
    "\n",
    "![](advanced_indexiing.png)\n",
    "\n",
    "Conversely, large chunks provide great context but perform poorly in retrieval because their core meaning gets diluted.\n",
    "\n",
    "`This is the classic “chunk size” dilemma.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e8b74a",
   "metadata": {},
   "source": [
    "### Multi-Representation Indexing\n",
    "\n",
    "The core idea of Multi-Representation Indexing is simple but powerful: instead of embedding the full document chunks, we create a smaller, more focused representation of each chunk (like a summary) and embed that instead.\n",
    "\n",
    "![](multi-representation.png)\n",
    "\n",
    "During retrieval, we search over these concise summaries. Once we find the best summary, we use its ID to look up and retrieve the full, original document chunk.\n",
    "\n",
    "This way, we get the precision of searching over small, dense summaries and the rich context of the larger parent documents for generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "512f6bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2 documents.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "docs = loader.load()\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2024-02-05-human-data-quality/\")\n",
    "docs.extend(loader.load())\n",
    "\n",
    "print(f\"Loaded {len(docs)} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4afb2a",
   "metadata": {},
   "source": [
    "Next, we’ll create a chain to generate a summary for each of these documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e1a2ebc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document \"LLM Powered Autonomous Agents\" by Lilian Weng provides an in-depth exploration of autonomous agents powered by large language models (LLMs). It outlines key components of such systems, including planning, memory, and tool use:\n",
      "\n",
      "1. **Agent System Overview**: LLMs are portrayed as the brain of autonomous agents, enabling them to perform complex tasks through various functionalities.\n",
      "\n",
      "2. **Planning**: This involves breaking down large tasks into manageable subtasks (task decomposition) and allowing the agent to self-reflect and learn from past actions to improve future performance.\n",
      "\n",
      "3. **Memory**: The document describes two types of memory—short-term (in-context learning) and long-term (stored information retrieved via vector databases). Maximum Inner Product Search (MIPS) serves as a method for efficient retrieval of stored data.\n",
      "\n",
      "4. **Tool Use**: It highlights the ability of LLMs to interact with external APIs and tools to extend their capabilities, integrating modules that can perform specific functions.\n",
      "\n",
      "5. **Challenges**: Key limitations include:\n",
      "   - Finite context length, which restricts the amount of historical information usable in decision-making.\n",
      "   - Difficulties in long-term planning and adapting to unexpected changes.\n",
      "   - Reliability concerns regarding the natural language interface, which can lead to errors.\n",
      "\n",
      "6. **Case Studies**: Examples illustrate practical applications of LLM-powered agents, such as:\n",
      "   - **ChemCrow**, which supports tasks in chemical synthesis and drug discovery.\n",
      "   - **Generative Agents**, which simulate human-like behavior in an interactive environment.\n",
      "\n",
      "7. **Proof-of-Concepts**: The document discusses various implementations like AutoGPT and GPT-Engineer that demonstrate the potential of LLM agents but also point to inherent reliability issues.\n",
      "\n",
      "The article emphasizes ongoing research challenges and opportunities for improvement in building robust and effective LLM-powered autonomous agents.\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "summary_chain = (\n",
    "    # Extract the page content from the document object\n",
    "    {\"doc\": lambda x: x.page_content}\n",
    "    | ChatPromptTemplate.from_template(\"Summarize the following document:\\n\\n{doc}\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "summaries = summary_chain.batch(docs, {\"max_concurrency\": 5}) # Use .batch() to run the summarization in parallel for efficiency\n",
    "\n",
    "print(summaries[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924d31e6",
   "metadata": {},
   "source": [
    "Now comes the crucial part. We need a `MultiVectorRetriever` which requires two main components:\n",
    "\n",
    "A `vectorstore` to store the embeddings of our summaries.\n",
    "A `docstore` (a simple key-value store) to hold the original, full documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "da003526",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asohi\\AppData\\Local\\Temp\\ipykernel_24624\\3401386783.py:6: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(collection_name=\"summaries\", embedding_function=OpenAIEmbeddings())\n"
     ]
    }
   ],
   "source": [
    "from langchain.storage import InMemoryByteStore\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# The vectorstore to index the summary embeddings\n",
    "vectorstore = Chroma(collection_name=\"summaries\", embedding_function=OpenAIEmbeddings())\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryByteStore()\n",
    "id_key = \"doc_id\" # This key will link summaries to their parent documents\n",
    "\n",
    "# The retriever that orchestrates the whole process\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    byte_store=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "\n",
    "# Generate unique IDs for each of our original documents\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]\n",
    "\n",
    "# Create new Document objects for the summaries, adding the 'doc_id' to their metadata\n",
    "summary_docs = [\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    for i, s in enumerate(summaries)\n",
    "]\n",
    "\n",
    "# Add the summaries to the vectorstore\n",
    "retriever.vectorstore.add_documents(summary_docs)\n",
    "\n",
    "# Add the original documents to the docstore, linking them by the same IDs\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "083329ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Result from searching summaries ---\n",
      "The document \"LLM Powered Autonomous Agents\" by Lilian Weng provides an in-depth exploration of autonomous agents powered by large language models (LLMs). It outlines key components of such systems, including planning, memory, and tool use:\n",
      "\n",
      "1. **Agent System Overview**: LLMs are portrayed as the brain of autonomous agents, enabling them to perform complex tasks through various functionalities.\n",
      "\n",
      "2. **Planning**: This involves breaking down large tasks into manageable subtasks (task decomposition) and allowing the agent to self-reflect and learn from past actions to improve future performance.\n",
      "\n",
      "3. **Memory**: The document describes two types of memory—short-term (in-context learning) and long-term (stored information retrieved via vector databases). Maximum Inner Product Search (MIPS) serves as a method for efficient retrieval of stored data.\n",
      "\n",
      "4. **Tool Use**: It highlights the ability of LLMs to interact with external APIs and tools to extend their capabilities, integrating modules that can perform specific functions.\n",
      "\n",
      "5. **Challenges**: Key limitations include:\n",
      "   - Finite context length, which restricts the amount of historical information usable in decision-making.\n",
      "   - Difficulties in long-term planning and adapting to unexpected changes.\n",
      "   - Reliability concerns regarding the natural language interface, which can lead to errors.\n",
      "\n",
      "6. **Case Studies**: Examples illustrate practical applications of LLM-powered agents, such as:\n",
      "   - **ChemCrow**, which supports tasks in chemical synthesis and drug discovery.\n",
      "   - **Generative Agents**, which simulate human-like behavior in an interactive environment.\n",
      "\n",
      "7. **Proof-of-Concepts**: The document discusses various implementations like AutoGPT and GPT-Engineer that demonstrate the potential of LLM agents but also point to inherent reliability issues.\n",
      "\n",
      "The article emphasizes ongoing research challenges and opportunities for improvement in building robust and effective LLM-powered autonomous agents.\n",
      "\n",
      "--- Metadata showing the link to the parent document ---\n",
      "{'doc_id': 'd6e850c0-fd3b-40fa-a00c-25e1255f8f58'}\n"
     ]
    }
   ],
   "source": [
    "query = \"Memory in agents\"\n",
    "\n",
    "# First, let's see what the vectorstore finds by searching the summaries\n",
    "sub_docs = vectorstore.similarity_search(query, k=1)\n",
    "print(\"--- Result from searching summaries ---\")\n",
    "print(sub_docs[0].page_content)\n",
    "print(\"\\n--- Metadata showing the link to the parent document ---\")\n",
    "print(sub_docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0d751531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- The full document retrieved by the MultiVectorRetriever ---\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LLM Powered Autonomous Agents | Lil'Log\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Lil'Log\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Posts\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Archive\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tags\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FAQ\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "Table of Contents\n",
      "\n",
      "\n",
      "\n",
      "Agent System Overview\n",
      "\n",
      "Component One: Planning\n",
      "\n",
      "Task Decomposition\n",
      "\n",
      "Self-Reflection\n",
      "\n",
      "\n",
      "Component Two: Memory\n",
      "\n",
      "Types of Memory\n",
      "\n",
      "Maximum Inner Product Search (MIPS)\n",
      "\n",
      "\n",
      "Component Three:\n"
     ]
    }
   ],
   "source": [
    "# Let the full retriever do its job\n",
    "retrieved_docs = retriever.get_relevant_documents(query, n_results=1)\n",
    "\n",
    "# Print the beginning of the retrieved full document\n",
    "print(\"\\n--- The full document retrieved by the MultiVectorRetriever ---\")\n",
    "print(retrieved_docs[0].page_content[0:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7140a014",
   "metadata": {},
   "source": [
    "### **Hierarchical Indexing (RAPTOR) Knowledge Tree**\n",
    "\n",
    "**The Theory:** RAPTOR (Recursive Abstractive Processing for Tree-Organized Retrieval) takes the multi-representation idea a step further. Instead of just one layer of summaries, RAPTOR builds a multi-level tree of summaries. It starts by clustering small document chunks. It then summarizes each cluster.\n",
    "\n",
    "![](raptor.png)\n",
    "\n",
    "Then, it takes these summaries, clusters them, and summarizes the new clusters. This process repeats, creating a hierarchy of knowledge from fine-grained details to high-level concepts. When you query, you can search at different levels of this tree, allowing for retrieval that can be as specific or as general as needed.\n",
    "\n",
    "This is a more advanced technique.\n",
    "\n",
    "Implementation: https://github.com/langchain-ai/langchain/blob/master/cookbook/RAPTOR.ipynb \n",
    "\n",
    "Paper: https://arxiv.org/pdf/2401.18059"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2a8cf6",
   "metadata": {},
   "source": [
    "### **Token-Level Precision (ColBERT)**\n",
    "\n",
    "**The Theory:** Standard embedding models create a single vector for an entire chunk of text (this is called a “bag-of-words” approach). This can lose a lot of nuance.\n",
    "\n",
    "![](colbert.png)\n",
    "\n",
    "`ColBERT (Contextualized Late Interaction over BERT) offers a more granular approach. It generates a separate, context-aware embedding for every single token in the document.`\n",
    "\n",
    "When you make a query, ColBERT also embeds every token in your query. Then, instead of comparing one document vector to one query vector, it finds the maximum similarity between each query token and any document token.\n",
    "\n",
    "This “late interaction” allows for a much finer-grained understanding of relevance, excelling at keyword-style searches.\n",
    "\n",
    "### ColBERT Late Interaction\n",
    "\n",
    "**ColBERT** makes search smarter by breaking your query and each document into their individual words (tokens), then giving each word its own vector (embedding) based on context.[1][4]\n",
    "\n",
    "Instead of checking if the entire query is similar to the entire document (like most models do), ColBERT matches **each word in your query** to **every word in the document**. For each query word, it finds the one document word that is most similar, keeping the highest score—this is the 'late interaction' step.[4][1]\n",
    "\n",
    "**Why is this better?**\n",
    "- ColBERT can spot relevant details, even if words are phrased differently in document and query.\n",
    "- It does a keyword-style search, but with context-aware vectors, so matches are smarter and more precise.[3]\n",
    "- It adds up the best matches for every query word, giving a fine-grained relevance score for each document.[4]\n",
    "\n",
    "**Summary:**\n",
    "- Every word in your query is compared to every word in each document.\n",
    "- ColBERT finds the best matching word for each query word (\"max similarity\").\n",
    "- These scores are summed up for a detailed relevance score.\n",
    "- This lets ColBERT find relevant results even for tricky, keyword-heavy searches.\n",
    "\n",
    "If you're used to models that give each document a single vector, ColBERT goes much deeper by making lots of smaller, meaningful comparisons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab77d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragatouille import RAGPretrainedModel\n",
    "\n",
    "RAG = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b0bc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_wikipedia_page(title: str):\n",
    "    \"\"\"A helper function to retrieve content from Wikipedia.\"\"\"\n",
    "    # Wikipedia API endpoint and parameters\n",
    "    URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "    params = { \"action\": \"query\", \"format\": \"json\", \"titles\": title, \"prop\": \"extracts\", \"explaintext\": True }\n",
    "    headers = {\"User-Agent\": \"MyRAGApp/1.0\"}\n",
    "    response = requests.get(URL, params=params, headers=headers)\n",
    "    data = response.json()\n",
    "    page = next(iter(data[\"query\"][\"pages\"].values()))\n",
    "    return page.get(\"extract\")\n",
    "\n",
    "full_document = get_wikipedia_page(\"Hayao_Miyazaki\")\n",
    "\n",
    "# Index the document with RAGatouille. It handles the chunking and token-level embedding internally.\n",
    "RAG.index(\n",
    "    collection=[full_document],\n",
    "    index_name=\"Miyazaki-ColBERT\",\n",
    "    max_document_length=180,\n",
    "    split_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9c6171",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = RAG.search(query=\"What animation studio did Miyazaki found?\", k=3)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b34f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the RAGatouille model into a LangChain-compatible retriever\n",
    "colbert_retriever = RAG.as_langchain_retriever(k=3)\n",
    "\n",
    "# Use it like any other retriever\n",
    "retrieved_docs = colbert_retriever.invoke(\"What animation studio did Miyazaki found?\")\n",
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2f8e02",
   "metadata": {},
   "source": [
    "## Advanced Retrieval & Generation\n",
    "\n",
    "![](advanced-retrieval-generation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9b804e",
   "metadata": {},
   "source": [
    "### Dedicated Re-ranking\n",
    "\n",
    "Standard retrieval methods give us a ranked list of documents, but this initial ranking isn’t always perfect. Re-ranking is a crucial second-pass step where we take the initial set of retrieved documents and use a more sophisticated (and often more expensive) model to re-order them based on their relevance to the query.\n",
    "\n",
    "![](reranking.png)\n",
    "\n",
    "`This ensures that the most relevant documents are placed at the very top of the context we provide to the LLM.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2bd91d",
   "metadata": {},
   "source": [
    "Now, we introduce the `ContextualCompressionRetriever`. This special retriever wraps our base retriever and adds a \"compressor\" step. Here, our compressor will be the `CohereRerank` model.\n",
    "\n",
    "It will take the 10 documents from our base retriever and re-order them, returning only the most relevant ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e59e2d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Re-ranked and Compressed Documents ---\n",
      "Relevance Score: 0.9988\n",
      "Content: Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chai...\n",
      "\n",
      "Relevance Score: 0.9988\n",
      "Content: Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chai...\n",
      "\n",
      "Relevance Score: 0.9988\n",
      "Content: Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chai...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "\n",
    "# 1. Load documents\n",
    "loader = WebBaseLoader(web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",))\n",
    "blog_docs = loader.load()\n",
    "\n",
    "# 2. Split into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300, chunk_overlap=50\n",
    ")\n",
    "splits = text_splitter.split_documents(blog_docs)\n",
    "\n",
    "# 3. Create vector store\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
    "\n",
    "# 4. Add reranker\n",
    "compressor = CohereRerank(model=\"rerank-english-v3.0\")  # use v3.0 or v3.5\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")\n",
    "\n",
    "# 5. Ask a question\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "compressed_docs = compression_retriever.get_relevant_documents(question)\n",
    "\n",
    "# 6. Print results\n",
    "print(\"--- Re-ranked and Compressed Documents ---\")\n",
    "for doc in compressed_docs:\n",
    "    print(f\"Relevance Score: {doc.metadata.get('relevance_score', 0):.4f}\")\n",
    "    print(f\"Content: {doc.page_content[:150]}...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42fd6ec",
   "metadata": {},
   "source": [
    "## Self-Correction using AI Agents\n",
    "\n",
    "What if our RAG system could check its own work before giving an answer? That’s the idea behind self-correcting RAG architectures like `CRAG (Corrective RAG)` and `Self-RAG`.\n",
    "\n",
    "![](self-correcting.png)\n",
    "\n",
    "These aren’t just simple chains, they are dynamic graphs (often built with LangGraph) that can reason about the quality of retrieved information and decide on a course of action.\n",
    "\n",
    "* **CRAG:** If the retrieved documents are irrelevant or ambiguous for a given query, a CRAG system won’t just pass them to the LLM. Instead, it triggers a new, more robust web search to find better information, corrects the retrieved documents, and then proceeds with generation.\n",
    "* **Self-RAG:** This approach takes it a step further. At each step, it uses an LLM to generate “reflection tokens” that critique the process. It grades the retrieved documents for relevance. If they’re not relevant, it retrieves again. Once it has good documents, it generates an answer and then grades that answer for factual consistency, ensuring it’s grounded in the source documents.\n",
    "\n",
    "These techniques represent the state-of-the-art in building reliable, production-grade RAG. Implementing them from scratch involves building a state machine or graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90cbe2f",
   "metadata": {},
   "source": [
    "## Impact of Long Context\n",
    "\n",
    "![](impact-long-context.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32218e7",
   "metadata": {},
   "source": [
    "## Manual RAG Evaluation\n",
    "\n",
    "1. **Faithfulness:** Does the answer stick strictly to the provided context? A faithful answer does not invent information or use the LLM’s pre-trained knowledge to answer. This is the single most important metric for preventing hallucinations.\n",
    "2. **Correctness:** Is the answer factually correct when compared to a “ground truth” or reference answer?\n",
    "3. **Contextual** Relevancy: Was the context we retrieved actually relevant to the user’s question? This evaluates the performance of our retriever, not the generator.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381fe0a7",
   "metadata": {},
   "source": [
    "### Building Evaluators from Scratch with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8860a713",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asohi\\Downloads\\RAG Ecosystem\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1900: UserWarning: Received a Pydantic BaseModel V1 schema. This is not supported by method=\"json_schema\". Please use method=\"function_calling\" or specify schema via JSON Schema or Pydantic V2 BaseModel. Overriding to method=\"function_calling\".\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# We'll use a powerful LLM like gpt-4o to act as our \"judge\" for reliable evaluation.\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\", max_tokens=4000)\n",
    "\n",
    "# Define the output schema for our evaluation score to ensure consistent, structured output.\n",
    "class ResultScore(BaseModel):\n",
    "    score: float = Field(..., description=\"The score of the result, ranging from 0 to 1 where 1 is the best possible score.\")\n",
    "\n",
    "# This prompt template clearly instructs the LLM on how to score the answer's correctness.\n",
    "correctness_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"ground_truth\", \"generated_answer\"],\n",
    "    template=\"\"\"\n",
    "    Question: {question}\n",
    "    Ground Truth: {ground_truth}\n",
    "    Generated Answer: {generated_answer}\n",
    "\n",
    "    Evaluate the correctness of the generated answer compared to the ground truth.\n",
    "    Score from 0 to 1, where 1 is perfectly correct and 0 is completely incorrect.\n",
    "    \n",
    "    Score:\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# We build the evaluation chain by piping the prompt to the LLM with structured output.\n",
    "correctness_chain = correctness_prompt | llm.with_structured_output(ResultScore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "fc1fefe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctness Score: 0.5\n"
     ]
    }
   ],
   "source": [
    "def evaluate_correctness(question, ground_truth, generated_answer):\n",
    "    \"\"\"A helper function to run our custom correctness evaluation chain.\"\"\"\n",
    "    result = correctness_chain.invoke({\n",
    "        \"question\": question, \n",
    "        \"ground_truth\": ground_truth, \n",
    "        \"generated_answer\": generated_answer\n",
    "    })\n",
    "    return result.score\n",
    "\n",
    "# Test the correctness chain with a partially correct answer.\n",
    "question = \"What is the capital of France and Spain?\"\n",
    "ground_truth = \"Paris and Madrid\"\n",
    "generated_answer = \"Paris\"\n",
    "score = evaluate_correctness(question, ground_truth, generated_answer)\n",
    "\n",
    "print(f\"Correctness Score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11483bc",
   "metadata": {},
   "source": [
    "Next, let’s build an evaluator for `Faithfulness`. This is arguably more important than correctness for RAG, as it’s our primary defense against hallucination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b6e07f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asohi\\Downloads\\RAG Ecosystem\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1900: UserWarning: Received a Pydantic BaseModel V1 schema. This is not supported by method=\"json_schema\". Please use method=\"function_calling\" or specify schema via JSON Schema or Pydantic V2 BaseModel. Overriding to method=\"function_calling\".\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# The prompt template for faithfulness includes several examples (few-shot prompting)\n",
    "# to make the instructions to the judge LLM crystal clear.\n",
    "faithfulness_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\",\"context\", \"generated_answer\"],\n",
    "    template=\"\"\"\n",
    "    Question: {question}\n",
    "    Context: {context}\n",
    "    Generated Answer: {generated_answer}\n",
    "\n",
    "    Evaluate if the generated answer to the question can be deduced from the context.\n",
    "    Score of 0 or 1, where 1 is perfectly faithful *AND CAN BE DERIVED FROM THE CONTEXT* and 0 otherwise.\n",
    "    You don't mind if the answer is correct; all you care about is if the answer can be deduced from the context.\n",
    "    \n",
    "    [... a few examples from the notebook to guide the LLM ...]\n",
    "\n",
    "    Example:\n",
    "    Question: What is 2+2?\n",
    "    Context: 4.\n",
    "    Generated Answer: 4.\n",
    "    In this case, the context states '4', but it does not provide information to deduce the answer to 'What is 2+2?', so the score should be 0.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Build the faithfulness chain using the same structured LLM.\n",
    "faithfulness_chain = faithfulness_prompt | llm.with_structured_output(ResultScore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7295866e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "def evaluate_faithfulness(question, context, generated_answer):\n",
    "    \"\"\"A helper function to run our custom faithfulness evaluation chain.\"\"\"\n",
    "    result = faithfulness_chain.invoke({\n",
    "        \"question\": question, \n",
    "        \"context\": context, \n",
    "        \"generated_answer\": generated_answer\n",
    "    })\n",
    "    return result.score\n",
    "\n",
    "# Test the faithfulness chain. The answer is correct, but is it faithful?\n",
    "question = \"what is 3+3?\"\n",
    "context = \"6\"\n",
    "generated_answer = \"6\"\n",
    "score = evaluate_faithfulness(question, context, generated_answer)\n",
    "\n",
    "print(f\"Faithfulness Score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64360dd8",
   "metadata": {},
   "source": [
    "### Evaluation with Frameworks\n",
    "\n",
    "![](evals.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba769e10",
   "metadata": {},
   "source": [
    "### **Rapid Evaluation with `deepeval`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "cff4616f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Correctness </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">[</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">GEval</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">]</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mCorrectness \u001b[0m\u001b[1;38;2;106;0;255m[\u001b[0m\u001b[38;2;106;0;255mGEval\u001b[0m\u001b[1;38;2;106;0;255m]\u001b[0m\u001b[38;2;106;0;255m Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Faithfulness Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">4.1</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mFaithfulness Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-\u001b[0m\u001b[1;38;2;55;65;81m4.1\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:00\u001b[0m0m\n",
      "\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:00\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:00\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:00\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:00\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:00\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:00\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:00\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:00\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:00\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:00\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:00\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:01\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:00\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:01\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:01\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:01\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:01\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:01\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:01\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:01\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:01\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:01\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:01\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:01\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:01\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:01\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:01\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:01\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:01\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:01\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:01\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:02\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:02\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:02\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:02\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:02\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:02\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:02\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:02\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:02\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:02\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:02\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:02\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:02\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:02\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:02\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:02\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:02\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:02\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:03\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:03\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:03\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:03\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:03\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:03\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:03\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:03\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:03\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:03\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:03\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:03\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:03\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:03\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:03\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:03\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:03\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:03\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:04\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:04\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:04\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:04\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:04\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:04\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:04\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:04\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:04\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:04\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:04\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:04\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:04\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:04\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:04\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:04\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:05\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:04\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:05\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:05\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:05\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:05\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:05\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:05\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:05\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:05\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:05\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:05\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:05\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:05\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:05\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:05\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:05\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:05\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:05\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:05\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:06\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:06\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:06\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:06\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:06\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:06\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:06\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:06\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:06\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:06\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:06\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:06\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:06\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:06\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:06\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:06\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:06\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:06\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:07\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:07\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:07\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:07\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:07\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:07\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:07\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:07\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:07\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:07\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:07\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:07\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:10\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:10\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:10\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:10\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:10\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:10\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:10\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:10\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:10\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:10\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:10\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:10\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:10\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:10\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:11\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:11\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:11\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:11\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:11\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:11\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:11\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:11\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:11\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:11\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:11\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:11\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:11\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:11\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:11\u001b[0m\n",
      "    🎯 Evaluating test case #0        \u001b[30m----------------------------\u001b[0m \u001b[96m  0%\u001b[0m \u001b[34m0:00:11\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[92m--------------\u001b[0m\u001b[30m \u001b[0m\u001b[30m-------------\u001b[0m \u001b[96m 50%\u001b[0m \u001b[34m0:00:11\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[92m--------------\u001b[0m\u001b[30m \u001b[0m\u001b[30m-------------\u001b[0m \u001b[96m 50%\u001b[0m \u001b[34m0:00:12\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[92m--------------\u001b[0m\u001b[30m \u001b[0m\u001b[30m-------------\u001b[0m \u001b[96m 50%\u001b[0m \u001b[34m0:00:12\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[92m--------------\u001b[0m\u001b[30m \u001b[0m\u001b[30m-------------\u001b[0m \u001b[96m 50%\u001b[0m \u001b[34m0:00:12\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[92m--------------\u001b[0m\u001b[30m \u001b[0m\u001b[30m-------------\u001b[0m \u001b[96m 50%\u001b[0m \u001b[34m0:00:12\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[92m--------------\u001b[0m\u001b[30m \u001b[0m\u001b[30m-------------\u001b[0m \u001b[96m 50%\u001b[0m \u001b[34m0:00:12\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[92m--------------\u001b[0m\u001b[30m \u001b[0m\u001b[30m-------------\u001b[0m \u001b[96m 50%\u001b[0m \u001b[34m0:00:12\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[92m--------------\u001b[0m\u001b[30m \u001b[0m\u001b[30m-------------\u001b[0m \u001b[96m 50%\u001b[0m \u001b[34m0:00:12\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[92m--------------\u001b[0m\u001b[30m \u001b[0m\u001b[30m-------------\u001b[0m \u001b[96m 50%\u001b[0m \u001b[34m0:00:12\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[92m--------------\u001b[0m\u001b[30m \u001b[0m\u001b[30m-------------\u001b[0m \u001b[96m 50%\u001b[0m \u001b[34m0:00:12\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[92m--------------\u001b[0m\u001b[30m \u001b[0m\u001b[30m-------------\u001b[0m \u001b[96m 50%\u001b[0m \u001b[34m0:00:13\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[92m--------------\u001b[0m\u001b[30m \u001b[0m\u001b[30m-------------\u001b[0m \u001b[96m 50%\u001b[0m \u001b[34m0:00:13\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[92m--------------\u001b[0m\u001b[30m \u001b[0m\u001b[30m-------------\u001b[0m \u001b[96m 50%\u001b[0m \u001b[34m0:00:13\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[92m--------------\u001b[0m\u001b[30m \u001b[0m\u001b[30m-------------\u001b[0m \u001b[96m 50%\u001b[0m \u001b[34m0:00:13\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[92m--------------\u001b[0m\u001b[30m \u001b[0m\u001b[30m-------------\u001b[0m \u001b[96m 50%\u001b[0m \u001b[34m0:00:13\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2KEvaluating 2 test case(s) in parallel \u001b[92m--------------\u001b[0m\u001b[30m \u001b[0m\u001b[30m-------------\u001b[0m \u001b[96m 50%\u001b[0m \u001b[34m0:00:13\u001b[0m\n",
      "\u001b[?25h\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Correctness [GEval] (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The Actual Output matches the Expected Output exactly, indicating that the Input was correctly processed. There are no discrepancies or differences to justify or document., error: None)\n",
      "  - ✅ Faithfulness (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The score is 1.00 because there are no contradictions listed. Great job staying faithful to the retrieval context!, error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: what is 3+3?\n",
      "  - actual output: 6\n",
      "  - expected output: 6\n",
      "  - context: None\n",
      "  - retrieval context: ['6']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ❌ Correctness [GEval] (score: 0.28451149021433825, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The Actual Output 'MadriD.' does not match the Expected Output 'Madrid is the capital of Spain.' exactly. The discrepancy is due to incorrect processing of the Input, as the Actual Output is incomplete and contains a typographical error with the capitalization of 'D'. The response fails to provide a complete sentence as expected., error: None)\n",
      "  - ✅ Faithfulness (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The score is 1.00 because there are no contradictions listed. Great job staying faithful to the retrieval context!, error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What is the capital of Spain?\n",
      "  - actual output: MadriD.\n",
      "  - expected output: Madrid is the capital of Spain.\n",
      "  - context: None\n",
      "  - retrieval context: ['Madrid is the capital of Spain.']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Correctness [GEval]: 50.00% pass rate\n",
      "Faithfulness: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "\n",
      "\u001b[96m✓\u001b[0m Evaluation completed 🎉! \u001b[1m(\u001b[0mtime taken: \u001b[1;36m13.\u001b[0m69s | token cost: \u001b[1;36m0.0121915\u001b[0m USD\u001b[1m)\u001b[0m\n",
      "» Test Results \u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m total tests\u001b[1m)\u001b[0m:\n",
      "   » Pass Rate: \u001b[1;36m50.0\u001b[0m% | Passed: \u001b[1;32m1\u001b[0m | Failed: \u001b[1;31m1\u001b[0m\n",
      "\n",
      " ==============================================================================\n",
      "== \n",
      "\n",
      "» What to share evals with your team, or a place for your test cases to live? ❤️\n",
      "🏡\n",
      "  » Run \u001b[1;32m'deepeval view'\u001b[0m to analyze and save testing results on \u001b[38;2;106;0;255mConfident AI\u001b[0m.\n",
      "\n",
      "\n",
      "test_results=[TestResult(name='test_case_1', success=True, metrics_data=[MetricData(name='Correctness [GEval]', threshold=0.5, success=True, score=1.0, reason='The Actual Output matches the Expected Output exactly, indicating that the Input was correctly processed. There are no discrepancies or differences to justify or document.', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.0023825, verbose_logs='Criteria:\\ncorrectness \\n \\nEvaluation Steps:\\n[\\n    \"Verify that the Input is correctly processed to produce the Expected Output.\",\\n    \"Check if the Actual Output matches the Expected Output exactly.\",\\n    \"Identify any discrepancies between the Actual Output and Expected Output and determine if they are due to incorrect processing of the Input.\",\\n    \"Ensure that any differences between Expected and Actual Output are justified and documented.\"\\n] \\n \\nRubric:\\nNone \\n \\nScore: 1.0'), MetricData(name='Faithfulness', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because there are no contradictions listed. Great job staying faithful to the retrieval context!', strict_mode=False, evaluation_model='gpt-4.1', error=None, evaluation_cost=0.003454, verbose_logs='Truths (limit=None):\\n[\\n    \"6\"\\n] \\n \\nClaims:\\n[\\n    \"6\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"idk\",\\n        \"reason\": \"The claim \\'6\\' is too vague and there is no information in the retrieval context to support or contradict it.\"\\n    }\\n]')], conversational=False, multimodal=False, input='what is 3+3?', actual_output='6', expected_output='6', context=None, retrieval_context=['6'], additional_metadata=None), TestResult(name='test_case_0', success=False, metrics_data=[MetricData(name='Correctness [GEval]', threshold=0.5, success=False, score=0.28451149021433825, reason=\"The Actual Output 'MadriD.' does not match the Expected Output 'Madrid is the capital of Spain.' exactly. The discrepancy is due to incorrect processing of the Input, as the Actual Output is incomplete and contains a typographical error with the capitalization of 'D'. The response fails to provide a complete sentence as expected.\", strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.002775, verbose_logs='Criteria:\\ncorrectness \\n \\nEvaluation Steps:\\n[\\n    \"Verify that the Input is correctly processed to produce the Expected Output.\",\\n    \"Check if the Actual Output matches the Expected Output exactly.\",\\n    \"Identify any discrepancies between the Actual Output and Expected Output and determine if they are due to incorrect processing of the Input.\",\\n    \"Ensure that any differences between Expected and Actual Output are justified and documented.\"\\n] \\n \\nRubric:\\nNone \\n \\nScore: 0.28451149021433825'), MetricData(name='Faithfulness', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because there are no contradictions listed. Great job staying faithful to the retrieval context!', strict_mode=False, evaluation_model='gpt-4.1', error=None, evaluation_cost=0.00358, verbose_logs='Truths (limit=None):\\n[\\n    \"Madrid is the capital of Spain.\"\\n] \\n \\nClaims:\\n[\\n    \"MadriD.\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"idk\",\\n        \"reason\": \"The claim \\'MadriD.\\' is ambiguous and does not provide enough information to determine if it agrees or contradicts the context.\"\\n    }\\n]')], conversational=False, multimodal=False, input='What is the capital of Spain?', actual_output='MadriD.', expected_output='Madrid is the capital of Spain.', context=None, retrieval_context=['Madrid is the capital of Spain.'], additional_metadata=None)] confident_link=None\n"
     ]
    }
   ],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import GEval, FaithfulnessMetric\n",
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "\n",
    "# Test case 1 - correctness\n",
    "test_case_correctness = LLMTestCase(\n",
    "    input=\"What is the capital of Spain?\",\n",
    "    expected_output=\"Madrid is the capital of Spain.\",\n",
    "    actual_output=\"MadriD.\",\n",
    "    retrieval_context=[\"Madrid is the capital of Spain.\"]  # needed for Faithfulness\n",
    ")\n",
    "\n",
    "# Test case 2 - faithfulness (added expected_output so GEval won't break)\n",
    "test_case_faithfulness = LLMTestCase(\n",
    "    input=\"what is 3+3?\",\n",
    "    expected_output=\"6\",     # ✅ added so GEval has ground truth\n",
    "    actual_output=\"6\",\n",
    "    retrieval_context=[\"6\"]\n",
    ")\n",
    "\n",
    "# Run evaluation\n",
    "evaluation_results = evaluate(\n",
    "    test_cases=[test_case_correctness, test_case_faithfulness],\n",
    "    metrics=[\n",
    "        GEval(\n",
    "            name=\"Correctness\",\n",
    "            model=\"gpt-4o\",\n",
    "            criteria=\"correctness\",\n",
    "            evaluation_params=[\n",
    "                LLMTestCaseParams.INPUT,\n",
    "                LLMTestCaseParams.EXPECTED_OUTPUT,\n",
    "                LLMTestCaseParams.ACTUAL_OUTPUT,\n",
    "            ],\n",
    "        ),\n",
    "        FaithfulnessMetric(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(evaluation_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8d5345",
   "metadata": {},
   "source": [
    "**Another Powerful Alternative with `grouse`**\n",
    "\n",
    "`grouse` is another excellent open-source option, offering a similar suite of metrics but with a unique focus on allowing deep customization of the \"judge\" prompts. This is useful for fine-tuning evaluation criteria for a specific domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "48559197",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:24<00:00, 24.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-03 14:32:00,172 - LLM Call Tracker - INFO - Cost: 0.1039$\n",
      "2025-10-03 14:32:00,172 - LLM Call Tracker - INFO - Cost: 0.1039$\n",
      "Grouse Faithfulness Score (0 or 1): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\asohi\\Downloads\\RAG Ecosystem\\.venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3860: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\asohi\\Downloads\\RAG Ecosystem\\.venv\\Lib\\site-packages\\numpy\\_core\\_methods.py:144: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "from grouse import EvaluationSample, GroundedQAEvaluator\n",
    "\n",
    "evaluator = GroundedQAEvaluator()\n",
    "\n",
    "unfaithful_sample = EvaluationSample(\n",
    "    input=\"Where is the Eiffel Tower located?\",\n",
    "    actual_output=\"The Eiffel Tower is located at Rue Rabelais in Paris.\",\n",
    "    expected_output=\"The Eiffel Tower is located on the Champ de Mars in Paris, France.\",  # ✅ required\n",
    "    references=[\n",
    "        \"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France\",\n",
    "        \"Gustave Eiffel died in his appartment at Rue Rabelais in Paris.\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "result = evaluator.evaluate(eval_samples=[unfaithful_sample]).evaluations[0]\n",
    "\n",
    "print(\"Grouse Faithfulness Score (0 or 1):\", result.faithfulness.faithfulness)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3186bcc",
   "metadata": {},
   "source": [
    "**Evaluation with `RAGAS`**\n",
    "While deepeval and grouse are great general-purpose evaluators, `RAGAS (Retrieval-Augmented Generation Assessment)` is a framework built specifically for evaluating RAG pipelines. It provides a comprehensive suite of metrics that measure every component of your system, from retriever to generator.\n",
    "\n",
    "It requires four key pieces of information for each test case:\n",
    "\n",
    "* **question:** The user's input query.\n",
    "* **answer:** The final answer generated by our RAG system.\n",
    "* **contexts:** The list of documents retrieved by our retriever.\n",
    "* **ground_truth:** The correct, reference answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "128aad6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare the evaluation data\n",
    "questions = [\n",
    "    \"What is the name of the three-headed dog guarding the Sorcerer's Stone?\",\n",
    "    \"Who gave Harry Potter his first broomstick?\",\n",
    "    \"Which house did the Sorting Hat initially consider for Harry?\",\n",
    "]\n",
    "\n",
    "# These would be the answers generated by our RAG pipeline\n",
    "generated_answers = [\n",
    "    \"The three-headed dog is named Fluffy.\",\n",
    "    \"Professor McGonagall gave Harry his first broomstick, a Nimbus 2000.\",\n",
    "    \"The Sorting Hat strongly considered putting Harry in Slytherin.\",\n",
    "]\n",
    "\n",
    "# The ground truth, or \"perfect\" answers\n",
    "ground_truth_answers = [\n",
    "    \"Fluffy\",\n",
    "    \"Professor McGonagall\",\n",
    "    \"Slytherin\",\n",
    "]\n",
    "\n",
    "# The context retrieved by our RAG system for each question\n",
    "retrieved_documents = [\n",
    "    [\"A massive, three-headed dog was guarding a trapdoor. Hagrid mentioned its name was Fluffy.\"],\n",
    "    [\"First years are not allowed brooms, but Professor McGonagall, head of Gryffindor, made an exception for Harry.\"],\n",
    "    [\"The Sorting Hat muttered in Harry's ear, 'You could be great, you know, it's all here in your head, and Slytherin will help you on the way to greatness...'\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "8d5908c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# 2. Structure the data into a Hugging Face Dataset object\n",
    "data_samples = {\n",
    "    'question': questions,\n",
    "    'answer': generated_answers,\n",
    "    'contexts': retrieved_documents,\n",
    "    'ground_truth': ground_truth_answers\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "29e867bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/12 [00:00<?, ?it/s]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating: 100%|██████████| 12/12 [00:29<00:00,  2.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          user_input  \\\n",
      "0  What is the name of the three-headed dog guard...   \n",
      "1        Who gave Harry Potter his first broomstick?   \n",
      "2  Which house did the Sorting Hat initially cons...   \n",
      "\n",
      "                                  retrieved_contexts  \\\n",
      "0  [A massive, three-headed dog was guarding a tr...   \n",
      "1  [First years are not allowed brooms, but Profe...   \n",
      "2  [The Sorting Hat muttered in Harry's ear, 'You...   \n",
      "\n",
      "                                            response             reference  \\\n",
      "0              The three-headed dog is named Fluffy.                Fluffy   \n",
      "1  Professor McGonagall gave Harry his first broo...  Professor McGonagall   \n",
      "2  The Sorting Hat strongly considered putting Ha...             Slytherin   \n",
      "\n",
      "   faithfulness  answer_relevancy  context_recall  answer_correctness  \n",
      "0           1.0          0.942253             1.0            0.970923  \n",
      "1           0.0          0.988045             1.0            0.719327  \n",
      "2           0.0          0.986732             1.0            0.972860  \n"
     ]
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    answer_correctness,\n",
    ")\n",
    "\n",
    "# 3. Define the metrics we want to use for evaluation\n",
    "metrics = [\n",
    "    faithfulness,       # How factually consistent is the answer with the context? (Prevents hallucination)\n",
    "    answer_relevancy,   # How relevant is the answer to the question?\n",
    "    context_recall,     # Did we retrieve all the necessary context to answer the question?\n",
    "    answer_correctness, # How accurate is the answer compared to the ground truth?\n",
    "]\n",
    "\n",
    "# 4. Run the evaluation\n",
    "result = evaluate(\n",
    "    dataset=dataset, \n",
    "    metrics=metrics\n",
    ")\n",
    "\n",
    "# 5. Display the results in a clean table format\n",
    "results_df = result.to_pandas()\n",
    "print(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
